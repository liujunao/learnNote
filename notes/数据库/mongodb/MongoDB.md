# 一、MongoDB 介绍

## 1. 简介

- 易于使用

  > - MongoDB 是一个面向文档的数据库
  > - 不采用关系模型是为了获得更好的扩展性
  > - 面向文档的方法能使用一条记录来表现复杂的层次关系

- 易于扩展

  > - 纵向扩展： 使用计算能力更强的机器
  > - 横向扩展： 通过分区将数据分散到更多机器上
  > - MongoDB 采用横向扩展
  > - MongoDB 能自动处理跨集群的数据和负载，自动重新分配文档，及将用户请求路由到正确的机器上

- 丰富的功能

  > - 索引： 支持二级索引，允许快速查询，且提供唯一索引、复合索引、地理空间索引及全文索引
  > - 聚合： 支持“聚合管道”，能通过简单的片段创建复杂的聚合，并通过数据库自动优化
  > - 特殊的集合类型： 支持存在时间有限的集合：保存某时刻过期数据；固定大小集合： 保存近期数据
  > - 文件存储： MongoDB 支持的一种易用协议，用于存储大文件和文件元数据

- 卓越的性能

  > - MongoDB 能对文档进行动态填充，也能预分配数据文件，利用额外空间来换取稳定的性能
  > - MongoDB 把尽可能多的内存用作缓存，试图为每次查询自动选择正确的索引

## 2. 基础知识

### 1. 文档

- 文档是键值对的有序集

- 文档的键是字符串，值可以是多种不同的数据类型

  > - 键不能含有 `\0(空字符)`，该字符用于表示键的结尾
  > - `. 与 $` 是保留字符，只能在特定环境下使用

- MongoDB 区别类型与大小写

### 2. 集合

- 集合是一组文档

#### 1. 动态模式

- 集合是动态模式的，即集合中的文档多种多样

- 应尽量将同种模式文档放在一个集合中：

  - 将文档区分放在同一集合中，便于管理

  - 提高查询文档的速度

  - 同种类型的文档集合中，数据更加集中

  - 可更有效的对集合进行索引

    > - 创建索引时，需要使用文档的附加结构
    > - 索引是按照集合来定义的

#### 2. 命名

集合使用名称进行标识，集合可满足下列条件的任意 UTF-8 字符串：

- 集合名不能的空字符串

- 集合名不能包含 `\0` 字符，该字符表示集合名的结束

- 集合名不能以 `system.` 开头，这是为系统集合保留的前缀

  > - `system.users` 保存着数据库用户信息
  > - `system.namespaces` 保存着所有数据库集合的信息

- 集合名不能包含保留字符 `$`

**子集合**： 使用 `.` 分隔不同命名空间的子集合

- **GridFS(一种存储大文件的协议)** 使用子集合来存储文件的元数据
- 大多数驱动程序提供了用于访问指定集合的子集合的语法糖

### 3. 数据库

- 多个文档组成集合，多个集合组成数据库
- 一个 MongoDB 实例可承载多个数据库，每个数据库拥有多个集合
- 数据库通过名称来标识

**命名**： 

- 不能是空字符串
- 只能使用 ASCII 中的字母和数字
- 应该全部小写
- 最多64字节

**系统默认数据库**：初始化的数据库保存数据库系统运行信息

- `Admin`： root库

  > 一些特定的服务器端命令只能从这个库运行

- `Local`： 不会被复制，用于存储不准备分布式保存的、只保存在本地(单服务器)的集合

- `Config`：MangoDB 用于分片设置时，用于保存分片相关的配置信息

### 4. MongoDB Shell

- mongoShell 是用于与 mongodb 交互的工具，mongoshell 其实就一个强大的 javascript 解释器，可运行任意 javascript 程序

### 5. 数据类型

- MongoDB 在保留 JSON 基本键值对基础上，添加了其他数据类型：

  - `null`： 用于表示空或者不存在的数据，如： `{"x":null}`

  - **布尔类型**： true或者false，如：`{"x":false}`

  - **数值**： shell 默认使用 64 位浮点型数值

    > 对于整型值，可使用 `NumberInt 或 NumberLong` 类，如： `{"x":NUmberInt("3")}`

  - **字符串**： UTF-8 字符串都可表示为字符串类型的数据

  - **日期**： 存储的是从新纪元以来经过的毫秒数，不存储时区，如： `{"x":new Date()}`

    > - 创建日期时，使用 `new Date()`
    > - 将构造函数作为函数进行调用时，返回的是日期的字符串表示

  - **正则表达式**： 与 JS 的正则表达式语法相同

  - **数组**： 数据列表或数据集可表示为数组，如： `{"x":["a","b","c"]}`

    > 数组是一组值，既可作为有序对象，也能作为无序对象来操作

  - **内嵌文档**： 文档可嵌套其他文档，即文档可作为键的值，被嵌套的文档作为父文档的值，如： `{"x":{"foo":"bar"}}`

  - **对象 id**： 一个 12 字节的 ID，是文档的唯一标识

    > - MongoDB 存储的文档必须有一个 `_id` 键，键的值可是任何类型，默认是 `ObjectId` 对象
    >
    > - `ObjectId` 使用 12 字节，是一个由 24 个十六进制数字组成的字符串
    >
    >   > - 前 4 个字节： 是从标准纪元开始的时间戳，单位为秒(与随后的5字节组合可提供秒级别的唯一性)
    >   > - 接下来 3 字节： 是所在主机的唯一标识符，通常是机器主机名的散列值
    >   > - 接下来2 字节：来自产生 ObjectId 的进程标识符(PID)
    >   > - 最后 3 字节：是一个自动增加的计数器，确保相同进程同一秒产生的 ObjectId 不同
    >   >
    >   > 前 9 字节可保证同一秒钟不同机器不同进程产生的 ObjectId 是唯一的

  - **二进制数据**： 是一个任意字节的字符串，不能直接在 shell 中使用

    > 若想将非 UTF-8 字符保存到数据库中，二进制数据的唯一方式

  - **代码**： 可包括任意 JS 代码，如： `{"x":function(){...}}`

## 3. 增删改查

### 1. 插入文档

- **单个插入**：`insert` 方法可向目标集合插入文档，如： `db.foo.insert({"bar":"baz"})`

- **批量插入**： `batchInsert` 函数可批量插入，如： `db.foo.batchInsert([{"_id":0},{"_id":1},{"_id":2}])`

  > - 适用于**将多个文档插入到一个集合**中，不能将多个文档插入到多个集合
  > - `mongoimport` 用于导入原始数据
  > - 若一个文档插入失败，则在该失败文档前的文档会成功插入到集合，失败文档后的文档会插入失败

- 插入校验： MongoDB 只对数据进行最基本的检查

  > - 检查文档的基本结构： 如：若没有 `_id` 字段，则自动添加
  >
  > - 检查大小： 文档必须小于 16 MB，为了防止不良的模式设计且保证性能一致
  >
  >   > `Object.bsonsize(doc)` 可查看 doc 文档的 BSON 大小(单位为字节)

### 2. 更新文档

- `update(param1,param2)`： 
  - `param1`： 查询文档，用于定位需要更新的目标文档
  - `param2`： 修改文档，用于说明要对找到的文档进行修改
  - 将第 4 个参数设为 true，可更新所有匹配的文档

- **更新修改器**：指定对文档中的某些字段进行更新，更新修改器是特殊的键，用来指定复杂的更新操作

  - `$set`：用来指定一个字段的值(若不存在，则先创建)

    > 如： `db.user.update({"name":"joe"},{"$set":{"book":"Green Eggs and Ham"}})`

  - `$inc`： 用来增加已有键的值(若不存在，则先创建)，只能用于整型、长整型、双精度浮点型

    > 如： 加 50 ： `db.games.update({"user","joe"},{"$inc":{"score":50}})`

  - `$push`： 会向已有的数组末尾加入一个元素，若没有则创建一个新的**数组**

  - `$each`： 与其他更新修改器组合使用可一次添加或修改多个元素

  - `$slice`： 限制数组中所包含的元素长度

  - `$sort`： 根据指定字段对数组的所有对象进行排序

  - `$ne 或 $addToSet`： 保证数组中的元素不重复

  - `$pop`： 可在数组任何一端删除元素

    > - `{"$pop":{"key":1}}`： 从数组末尾删除一个元素
    > - `{"$pop":{"key":-1}}`： 从数组头部删除一个元素

  - `$pull`： 可根据特定条件来删除元素

- `upsert`： 特殊的更新，若没有找到符合更新条件的文档，就会以这个条件和更新文档为基础创建一个新的文档

- `findAndModify`： 可得到被更新的文档

  > 可使用字段：
  >
  > - `findAndModify`： 字符串，集合名
  > - `query`： 查询文档，用于检索文档的条件
  > - `sort`： 排序结果的条件
  > - `update`： 修改器文档，用于对匹配的文档进行更新
  > - `remove`： 布尔类型，表示是否删除文档(remove 与 update 必须指定一个)
  > - `new`： 布尔类型，表示返回更新前的文档还是更新后的文档，默认是更新前的文档
  > - `fields`： 文档中方需要返回的字段(可选)
  > - `upsert`： 布尔类型，值为 true 时表示这是一个 upsert，默认为 false

### 3. 删除文档

- `db.foo.remove()`： 删除 foo 集合中的所有文档，不会删除集合本身，也不会删除集合的元信息
- `db.foo.remove(xxx)`： 只有符合条件的可选参数的文档才会被删除，如： 删除 foo 集合中为 "opt" 为 true 的人： `db.opt.remove("opt":true)`
- `db.foo.drop()`： 整个集合被删除，所有元数据也都会不见
- `delete xxx`： 同 remove
- 删除数据是永久性的，不能撤销，也不能恢复

### 4. 查询

- `find()`： 返回一个集合中文档的子集

  > - 第一个参数： 是一个文档，用于指定查询条件，决定要返回哪些文档
  >
  >   > 空的査询文档会匹配集合的全部内容，默认是 `{}`
  >
  > - 第二个参数： 指定需要返回的键，如： `db.users.find({},{"name":1,"email":1})`

- `$lt, $lte, $gt, $gte, $ne 对应 <, <=, >, >=, !=` 

  > 如： 查询 18~30岁用户： `db.users.find({"age":{"$gte":18,"$lte":30}})` 

- `$in`： 用来查询一个键的多个值；`$nin`： 返回与数组中所有条件都不匹配的文档

-  `$or`： 可以在多个键中查询任意的给定值

- `$mod`： 会将查询的值除以第一个给定的值，若玉树等于第二个给定值则匹配成功

  > 如： `db.users.find({"id_num":{"$mod":[5,11]}})`

- `$not, $and, $or, $nor`： 是元条件句，即可用在其他条件之上；与正则表达式联合使用时很有用，可用来查找与特定模式不匹配的文档

  > 如：  `db.users.find({"id_num":{"$not":{"$mod":[5,11]}}})`

- `$exists`： 判断键值是否已存在

  > 如： 判断 `null`： `db.c.find({"z":{"$in":[null],"$exists":true}})`

- `$all`： 可通过多个元素来匹配数组

- `$size`： 用来查询特定长度的数组

- `$elemMatch`： 要求同时使用查询条件中的两个语句与一个数组元素进行比较，不匹配非数组元素

- **查询内嵌文档**： 若要查询一个完整子文档，则必须精确匹配

  > 如： `db.blog.find({"comments" : {"$elemMatch" : {"author" : "joe","score":{"$gte":5}}}})`

- `$where`： 可在查询中执行任意的 JS(为安全起见，应限制或禁止对 $where 的使用)

### 5. 写入安全机制

- **写入安全**： 一种客户端设置，用于控制写入的安全级别

  > - 默认情况下，插入、删除和更新都会一直等待数据库响应（写入是否成功）， 然后才会继续执行
  >
  > - 通常，遇到错误时，客户端会抛出一个异常

- 两种最基本的写入安全机制

  - 应答式写入： 默认的方式，数据库会给出响应，告诉你写入操作是否成功执行

  - 非应答式写入： 不返回任何响应，但仍会做错误检查

    > - shell 在执行非应答式写入后，会检査最后一个操作是否成功，然后才会向用户输出提示信息
    >
    > - 如果在集合上执行了一系列无效操作，最后又执行了一个有效操作，shell 并不会提示有错误发生

## 4. 游标

- 数据库使用游标返回 find 的执行结果，客户端对游标的实现通过能对最终结果进行有效控制

### `limit,skip,sort`

- 限制： `db.c.find().limit(3)`
- 略过： `db.c.find().skip(3)`
- 排序： `db.c.find().sort({username:1,age:-1})`

# 二、设计与应用

## 1. 索引

### 1. 索引简介

- **全表扫描**： 不使用索引的査询

- **复合索引**： 建立在多个字段上的索引

  > 建立方式： `db.users.ensureIndex({"age" : 1, "username" : 1})`

- **索引嵌套文档**： 在嵌套文档的键上建立索引，方式与正常键相同

- 索引数组： 对数组建立索引

  > 可以高效的搜索数组中的特定元素

- 多键索引： 索引的键在某个文档中是一个数组

  > - `explain()` 的输出的字段 `isMultikey` 的值为 true
  > - 索引只要被标记为多键索引，就无法再改变

- **索引基数**： 集合中某索引拥有不同值的数量

- `explain()`： 能提供大量与查询相关的信息，可在任意查询后添加 `explain()` 调用

### 2. 索引使用

- **索引的使用**：在提取较小的子数据集时，索引很高效 ；结果集中所占比例越大，索引速度越慢

  > **使用索引需要进行两次查找**：
  >
  > - 第一次： 查找索引条目
  > - 第二次： 根据索引指针查找相应的文档

  ​						**影响索引效率的属性**

| 索引通常适用的情况 | 全表扫描通常适用的情况 |
| :----------------: | :--------------------: |
|      集合较大      |        集合较小        |
|      文档较大      |        文档较小        |
|     选择性査询     |      非选择性査询      |

### 3. 索引类型

> 索引大小不能超过 1024 字节

- **唯一索引**： 可确保集合的每个文档的指定键都有唯一值

  > - 创建唯一索引： `db.users.ensureIndex({"username" : 1},"unique":true)`
  >
  > - 当插入重复文档时，程序会抛出异常
  >
  > - 当在已有重复值的集合中创建唯一索引时，会失败
  >
  >   > 可使用 dropDups 强制建立唯一索引： `db.people.ensureindex({"username" : 1}, {"unique" : true, "dropDups" : true})`

- **稀疏索引**： 只是不需要将每个文档都作为索引条目，`spare` 选项可创建稀疏索引

  > - 稀疏索引不必唯一
  > - 创建稀疏索引： `db.people.ensureindex({"username" : 1}, {"sparse" : true})`

### 4. 索引管理

- 数据库索引信息存储在 `system.indexes` 集合中 

  > - 不能再其中插入或删除文档，只能通过 `ensureIndex 或 dropIndexes` 对其进行操作
  > - 可执行 `db.collectionName.getIndexes()` 查看给定集合上的所有索引信息

- **标识索引**： 集合中的每一个索引都有一个名称，用于唯一标识这个索引，也可以用于服务器端来删除或者操作索引

  > 索引名称的默认形式是 `keyname1_dir1_keyname2_dir2_..._keynameN_dirN`，其中 `keynameX`是索引的键，`dirX` 是索引的方向

- **修改索引**： `dropIndex` 命令可删除索引

## 2. 特殊索引与集合

### 1. 固定集合

- **普通集合**： 是动态创建的，且可以自动增长以容纳更多的数据

- **固定集合**： 需事先创建好，且大小固定

  > - 当固定集合被占满时，如果再插入新文档，固定集合会自动将最老的文档从集合中删除
  > - 数据被顺序写入磁盘上的固定空间，因此在碟式磁盘上的写入速度非常快，尤其是集合拥有专用磁盘时
  > - 固定集合可以用于记录日志

- **创建固定集合**： 在 shell 中使用 `createCollection` 函数创建

  > - 创建了一个名为 my_collection 大小为100 000 字节的固定集合： `db.createCollection("my_collection", {"capped":true,"size":100000});`
  >
  > - 命令 `convertToCapped`： 将已有的某个常规集合转换为固定集合
  >
  >   > 将 test 集合转换为一个大小为 10000 字节的固定集合：`db.runCommand({"convertToCapped':"test","size":10000}};`

- 对固定集合进行**自然排序**：结果集中文档的顺序就是文档在磁盘上的顺序

  > 排序事例： `db.my_collection.find().sort({"$natural":-1})`

- **循环游标**： 一种特殊的游标，当循环游标的结果集被取光后，游标不会被关闭

  > - 当有新文档插入到集合中时，循环游标会继续取到结果
  > - 循环游标只能用在固定集合上
  > - 常用于当文档被插入到固定集合时对新插入的文档进行处理
  > - 超过10 分钟没有新的结果，循环游标就会被释放

### 2. 特殊索引

#### 1. TTL 索引

- **TTL 索引(具有生命周期的索引)**： 允许为毎一个文档设置一个超时时间，一个文档到达预设置的老化程度之后就会被删除

  > 这种类型的索引对于缓存问题（比如会话的保存）非常有用

- 在 `ensureIndex` 中指定 `expireAfterSecs` 选项就可以创建一个 TTL 索引：

  ```js
  //在 lastUpdated 字段上建立一个 TTL 索引，且超时时间为 24h
  db.foo.ensureIndex({"lastUpdated":1},{"expireAfterSecs":60*60*24})
  ```

- MongoDB 毎分钟对 TTL 索引进行一次清理，所以不应该依赖以秒为单位的时间保证索引的存活状态

- 命令 `collMod`： 可修改 expireAfterSecs 的值： `db.runCommand({"collMod":"someapp.cache","expireAfterSecs":3600})`

- 一个集合上刻有多个 TTL 索引；TTL 索引不能是复合索引，但是可以像普通索引一样用来优化排序和査询

#### 2. 全文本索引

- **全文索引**： 用于在文档中捜索文本

  > 创建全文本索引的成本很高，所以应在离线状态下创建全文本索引，或在对性能没要求时

- **全文本索引会降低分片时的数据迁移速度**：将数据迁移到其他分片时，所有文本都需要重新进行索引
- 命令 `setParameter`： 可用于启用全文本索引： `db.adminCommand({"setParameter":1,"textSearchEnabled":true})`
- 全文本索引**只会对字符串数据进行索引**
- 一个集合上最多只能有一个全文本索引，但是全文本索引可以包含多个字段

#### 3. 地理空间索引

- **2dsphere 索引(用于地球表面类型的地图)**： 允许使用GeoJSON 格式指定点、线和多边形

  > - 点可以用形如[longitude, latitude] ([经度，纬度]) 的两个元素的数组表示
  > - 线可以用一个由点组成的数组来表示
  > - 多边形的表示方式与线一样(都是一个由点组成的数组)， 但是 `type` 不同
  - **不同类型的地理空间査询**：交集、包含、接近

    > 建议在用于表示地理位置的字段上建立地理空间索引，这样可以显著提高査询速度

    - 操作符 `$geoIntersects`： 用于找出与查询位置相交的文档
    - 操作符 `$within`： 用于查询完全包含在某个区域的文档
    - 操作符 `$near`： 用于查询附近的位置

- **2d 索引(用于平面地图和时间连续的数据)**： 只能对点进行索引，可以保存一个由点组成的数组，但是它只会被保存为由点组成的数组，不会被当成线

  - `$box`： 可构造矩形，如： `db.hyrule.find({"tile":{"$within":{"$box":[[10,20],[15,30]]}}})`

    > 接受两个元素：
    >
    > - 第一个元素： 指定左下角的坐标
    > - 第二个元素： 指定右上角的坐标

  - `$center`： 返回圆形范围内的所有文档，如： `db.hyrule.find({"tile":{"$within":{"$center":[[10,20],5]}}})`

    > 也接受一个两元素数组作为参数：
    >
    > - 第一个元素： 是一个点，用于指定圆心
    > - 第二个参数： 用于指定半径

### 3. GridFS

- `GridFS`： MongoDB 的一种存储机制，用来存储大型二进制文件

- **优点**：

  - 使用 `GridFS` 能简化你的栈

    > 如果已经在使用 MongoDB，那么可以使用 GridFS 来代替独立的文件存储工具

  - GridFS 会自动平衡已有的复制或者为 MongoDB 设置自动分片

    > 所以对文件存储做故障转移或者横向扩展会更容易

  - 当用于存储用户上传的文件时，GridFS 可以比较从容地解决其他一些文件系统可能会遇到的问题

    > 例如： 在 GridFS 文件系统中，如果在同一个目录下存储大量的文件，没有任何问题

  - 在 GridFS 中，文件存储的集中度会比较高，因为 MongoDB  是以 2GB 单位来分配数据文件的

- **缺点**： 

  - GridFS 的性能比较低：从 MongoDB 中访问文件，不如直接从文件系统中访问文件速度快

  - 如果要修改 GridFS 上的文档，只能先将已有文档删除，然后再将整个文档重新保存

    > MongoDB 将文件作为多个文档进行存储，所以无法在冋一时间对文件中的所有块加锁

## 3. 聚合

### 1. 聚合框架

- 使用聚合框架可以对集合中的文档进行变换和组合

- 可以用多个构件创建一个管道，用于对一连串的文档进行处理

  > 构件包括： 筛选、投射、分组、排序、限制、跳过

- 下面每一步都对应聚合框架中的一个操作符：

  - `{"$project":{"author" : 1}}`： 可以将 "author" 从毎个文档中投射出来

    > 与査询中的字段选择器比较像：
    >
    > - 可以通过指定 `"fieldname":1` 选择需要投射的字段
    >
    > - 也可通过指定 `"fieldname":0` 排除不需要的字段
    >
    > 执行完 `$project` 操作后，结果集中的每个文档都会以 `{"_id":id,"author":"authorName"}` 这样的形式表示

  - `{"$group":{"_id":"$author","count":{"$sum" : 1}}}`： 将作者按照名字排序，某个作者的名字每出现一次，就会对这个作者的 "count" 加 1
  - `{"$sort" : {"count" : -1}}`： 会对结果集中的文档根据 "count" 字段进行降序排列
  - `{"$limit" : 5}`： 将最终的返回结果限制为当前结果中的前5 个文档

### 2. 管道操作符

- 每个操作符都会接受一连串的文档，对这些文档做一些类型转换，最后将转换后的文档作为结果传递给下一个操作符（对于最后一个管道操作符，是将结果返回给客户端）
- 不同的管道操作符可以按任意顺序组合在一起使用，而且可以被重复任意多次

#### 1. `$match`

- `$match`： 用于对文档集合进行筛选，之后就可以在筛选得到的文档子集上做聚合
- 不能在 `$match` 中使用地理空间操作符
- 尽可能放在管道的前面位置：
  - 一是可以快速将不需要的文档过滤掉，以减少管道的工作量
  - 二是如果在投射和分组之前执行 `$match` ，查询可以使用索引

#### 2. `$project`

- `$project`： 可以从子文档中提取字段，可以重命名字段，还可以在字段上进行一些其他操作

- 比如操作：

  - 从文档中选择想要的字段，可以指定包含或者不包含一个字段

    > 返回的结果文档中只包含一个"author" 字段： `db.users.aggregate({"$project":{"author":1,"_id":0}})`

##### 1. 管道表达式

如 `$project`

##### 2. 数学表达式

>  算术表达式可用于操作数值

- `"$add": [expr1[,expr2,...,exprN]]`： 这个操作符接受一个或多个表达式作为参数，将这些表达式相加
- `"$subtract":[expr1,expr2]`： 接受两个表达式作为参数，用第一个表达式减去第二个表达式作为结果
- `"$multiply":[expr1[,expr2,...,exprN]]`： 接受一个或者多个表达式，并且将它们相乘
- `"$divide":[expr1,expr2]`： 接受两个表达式，用第一个表达式除以第二个表达式的商作为结果
- `"$mod":[expr1,expr2]`： 接受两个表达式，将第一个表达式除以第二个表达式得到的余数作为结果

##### 3. 日期表达式

- 只能对日期类型的字段进行日期操作，不能对数值类型字段做日期操作

- 每种日期类型的操作都是类似的：接受一个日期表达式，返回一个数值
- 日期表达式： `$year, $month, $week, $dayOfMonth, $dayOfWeek, $dayOfYear, $hour, $minute, $second`

##### 4. 字符串表达式

- `"$substr":[expr,startOffset,numToReturn]`： 会截取字符串 `expr` 的子串，从
  第 `startOffset` 字节开始的 `numToReturn` 字节
- `"$concat":[expr1[,expr2,...,exprN]]`： 将给定的表达式（或者字符串） 连接起来并返回结果
- `"$toLower":expr`： 返回字符串 `expr` 的小写形式
- `"$toUpper":expr`： 返回字符串 `expr` 的大写形式

##### 5. 逻辑表达式

> 用于控制语句

- `"$cmp":[expr1,expr2]`： 比较大小，若 `expr1 == expr2`，返回 0；若 `expr1 < expr2`，返回负数；若 `expr1 > expr2`，返回正数
- `"$strcasecmp":[string1,string2]`： 比较 string1 和 string1(区分大小写)，只对罗马字符有效
- `"$eq/$ne/$gt/$gte/$lt/$lte":[expr1,expr2]`： 执行相应的比较，并返回结果(true 或 false)
- `"$and":[expr1[,expr2,...,exprN]]`： 如果所有表达式的值都是true，返回true, 否则返回false
- `"$or":[expr1[,expr2,...,exprN]]`：只要有任意表达式的值为true，返回true，否则返false
- `"$not":expr`： 对 expr 取反
- `"$cond":[booleanExpr,trueExpr,falseExpr]`： 如果 booleanExpr 的值是 true，那就返回 trueExpr，否则返回 falseExpr
- `"$ifNull":[expr,replacementExpr]`： 如果 expr 是 null，返回 replacementExpr，否则返回 expr



#### 3. `$group`

- `$group`： 将文档依据特定字段的不同值进行分组

  > 如果选定了需要进行分组的字段，就可以将选定的字段传递给`$group` 函数的 `_id` 字段，如： `{"$group":{"_id":"$day"}}`

##### 1. 分组操作符

- 分组操作符允许对每个分组进行计算，得到相应的结果

##### 2. 算术操作符

用于对数值类型字段的值进行计算：

- `"$sum":value`： 对于分组中的每一个文档，将 value 与计算结果相加
- `"$avg":value`： 返回每个分组的平均值

##### 3. 极值操作符

下面的四个操作符可用于得到数据集合中的 ”边缘“值：

- `"$max":expr`： 返回分组内的最大值

- `"$min":expr`： 返回分组内的最小值

- `"$first":expr`： 返回分组的第一个值，忽略后面所有值

  > 只有排序之后，明确知道数据顺序时这个操作才有意义

- `"$last":expr`： 与 `"$first"` 相反，返回分组的最后一个值

> `"$max","$min" ` 会查看每一个文档，以便得到极值

##### 4. 数组操作符

- `"$addToSet":expr`： 如果当前数组中不包含 expr，就将它添加到数组中

  > 在返回结果集中，每个元素最多只出现一次，且元素的顺序是不确定的

- `"$push":expr`： 将 `expr` 添加到数组中，返回包含所有值的数组

##### 5. 分组行为

- `$group`： 只有等到收到所有的文档后，才能对文档进行分组，然后将各个分组发给管道中的下一个操作符

#### 4. `$unwind`

- `$unwind`: 可以将数组中的毎一个值拆分为单独的文档

#### 5. `$sort`

- `$sort`： 可以根据任何字段(或多个字段)进行排序

#### 6. `$limit`

- `$limit`： 接受一个数字 n，返回结果集中的前 n 个文档

#### 7. `$skip`

- `$skip`： 接受一个数字 n，丢弃结果集中的前 n 个文档，将剩余文档作为结果返回

### 3. MapReduce

- MapReduce： 当无法使用聚合框架的査询语言来解决的问题时，可使用MapReduce

- MapReduce 使用 JavaScript 作为 “查询语言”，因此能够表达任意复杂的逻辑，但速度很慢

- MapReduce 能够在多台服务器之间并行执行

  > 它会将一个大问题拆分为多个小问题，将各个小问题发送到不同的机器上，每台机器只负责完成一部分工作

### 4. 聚合命令

#### 1. `count`

- `count`： 最简单的聚合工具，用于返回集合中的文档数量，如： `db.foo.count()` 

#### 2. `distinct`

- `distinct`： 用来找出给定键的所有不同值，使用时必须指定集合和键，如： `db.runCommand({"distinct" : "people", "key" : "age"})`

#### 3. `group`

- `group`： 先选定分组所依据的键，而后 MongoDB 就会将集合依据选定键的不同值分成若干组，然后可以对每一个分组内的文档进行聚合，得到一个结果文档

## 4. 应用程序设计

### 1. 范式化和反范式化

- **范式化**： 将数据分散到多个不同的集合，不同集合之间可以相互引用数据

  > 范式化能够提高数据写入速度

- **反范式化**：将每个文档所需的数据都嵌入在文档内部，每个文档都拥有自己的数据副本

  > 反范式化能够提高数据读取速度



​							**内嵌数据与引用数据的比较**

|            更适合内嵌            |       更适合引用       |
| :------------------------------: | :--------------------: |
|            子文档较小            |       子文档较大       |
|         数据不会定期改变         |      数据经常改变      |
|         最终数据一致即可         | 中间阶段的数据必须一致 |
|         文档数据小幅增加         |    文档数据大幅增加    |
| 数据通常需要执行二次査询才能获得 | 数据通常不包含在结果中 |
|             快速读取             |        快速写入        |

- **基数**： 一个集合中包含的对其他集合的引用数量，常见的关系有一对一、一对多、多对多

### 2. 优化数据操作

- 如果要优化应用程序，首先必须知道对读写性能进行评估以便找到性能瓶颈

  - 对**读取操作**的优化： 通常包括正确使用索引，以及尽可能将所需信息放在单个文档中返回

  - 对**写入操作**的优化： 通常包括减少索引数量以及尽可能提髙更新效率

- **更新数据**时，需要明确更新是否会导致文件体积增长，以及增长程度

  > - 如果增长程度是可预知的，可以为文档预留足够的增长空间，这样可以避免文档移动，可以提高写入速度
  > - 检査一下填充因子：如果它大约是1.2 或者更大，可以考虑手动填充
  > - 如果要对文档进行手动填充，可以在创建文档时创建一个占空间比较大的字段，文件创建成功之后再将这个字段移除，这样就提前为文档分配了足够的空间供后续使用
  > - 如果文档中有一个字段需要增长，应该尽可能将这个字段放在文档最后的位置

- **删除旧数据**：

  - **使用固定集合**： 将集合大小设为一个比较大的值，当集合被填满时，将旧数据从固定集合中挤出

    > - 固定集合会对操作造成一些限制，而且在密集插入数据时会大大降低数据在固定集合内的存活期

  - **使用 TTL 集合**： 可以更精确地控制删除文档的时机

    > - 对于写入量很大的集合来说，该方式可能不够快：它通过遍历 TTL 索引来删除文档
    > - 若 TTL 集合能承受足够的写入量，使用 TTL 集合删除旧数据可能最简单

  - **定期删除集合**： 使用多个集合，每个月的文档单独使用一个集合，每当月份变更时，应用程序就开始使用新月份的集合，査询时要对当前月份和之前月份的集合都进行査询

    > - 可以应对任意的操作量
    > - 但对于应用程序来说会比较复杂，因为需要使用动态的集合名称，也要动态处理对多个数据库的査询

- 保存文档的集合设计： 具有相近模式的文档应该放在相同的集合中

  > MongoDB 不允许使用多个集合进行数据组合，如果有些文档需要进行集中査询或者聚合，那么这些文档应该放在同一个大集合里

- 保存文档的数据库设计： 按照重要性进行拆分，分为三个数据库：log(日志)、activities(活动)、users(用户)

  > - 好处： 最重要的数据集合的数据量可能最小
  > - MongoDB 通常不允许直接将数据从一个数据库移到另一个数据库

### 3. 数据一致性

- **MongoDB 内部机制**： 服务器为每个数据库连接维护一个请求队列，客户端每次发来的新请求都会添加到队列的末尾；入队之后，这个连接上的请求会依次得到处理，一个连接拥有一个一致的数据库视图，可以总是读取到这个连接最新写入的数据

  > - 每个列队只对应一个连接

### 4. 模式迁移

数据库模式可能需要相应地增长和改变：

- 最简单的方式：在应用程序需要时改进数据库模式，以确保应用程序能够支持所有旧版的模式

  > 这种方式可能会导致混乱，尤其是不同版本的模式之间有冲突时

- 结构化点的方案： 在毎个文档中包含一个 `version 或 v` 字段，使用这个字段来决定应用程序能够接受的文档结构

  > 这种方式对模式的要求更加严格：文档必须对多个版本都有效，且仍然需要支持各种旧版本

- 最后一种方式： 当模式发生变化时，将数据进行迁移

  > - MongoDB 允许使用动态模式，以避免执行迁移，因为执行迁移会对系统造成很大的压力
  > - MongoDB 中的多文档更新并不是原子的

### 5. 不适合 MongoDB 场景

- MongoDB 不支持事务，对事务性有要求的应用程序不建议使用
- MongoDB 不支持在多个不同维度上对不同类型的数据进行连接
- 若你的工具不支持 MongoDB，则应该考虑选择一个关系型数据库

# 三、复制

## 1. 创建副本集

- **复制**： 可以将数据副本保存到多台服务器上

- **副本集**： 是一组服务器

  - 一个主服务器：  用于处理客户端请求

  - 多个备份服务器： 用于保存主服务器的数据副本

    > 如果主服务器崩溃了，备份服务器会自动将其中一个成员升级为新的主服务器

### 1. 建立副本集

```shell
# 启动 mongodb 且不连接到任何 mongodb
mongo --nodb
# 创建一个包含三个服务器的副本集： 一个主服务器和两个备份服务器
replicaSet = new ReplSetTest({"nodes" : 3})
# 启动3个 mongod 进程
replicaSet.startSet()
# 配置复制功能
replicaSet.initiate()

# 新开一个 shell
# 在第二个 shell 中，连接到运行在 31000 端口的 mongod
connl = new Mongo("localhost:31000")
primaryDB = connl.getDB("test")
# 执行 isMaster 命令，来看副本集的状态:
primaryDB.isMaster()

# 执行一些操作
# 插入1000个文档
for (i=0; i<1000; i++){ primaryDB.coll.insert({count: i})}
# 检杳集合的文档数傲，确保真的插入成功了
primaryDB.coll.count()
# 检査其中一个副本集成员，验证一下其中是否有刚刚写入的那些文档的副本
conn2 = new Mongo("localhost:31001")
secondaryDB = conn2.getDB("test")
# 从备份节点读取数据
conn2.setSlaveOk() # 对连接进行设置，确保数据读取没问题
secondaryDB.coll.find()
# 不能对备份节点执行写操作：备份节点只通过复制功能写入数据，不接受客户端的写入请求
secondaryDB.coll.insert({"count" : 1001})
secondaryDB.runCommand({"getLastError" : 1})

# 自动故障转移：如果主节点挂了，其中一个备份节点会自动选举为主节点
# 先关掉主节点
primaryDB.adminCommand({"shutdown" : 1})
# 在备份节点上执行 isMaster，看看新的主节点是哪一个
secondaryDB.isMaster()

# 执行下面的命令关闭副本集
replicaSet.stopSet()
```

有几个关键的槪念需要注意：

- 客户端在单台服务器上可以执行的请求，都可以发送到主节点执行（读、写、执行命令、创建索引等）

- 客户端不能在备份节点上执行写操作

- 默认情况下，客户端不能从备份节点中读取数据

  > 在备份节点上显式地执行 `setSlaveOk` 之后，客户端就可以从备份节点中读取数据了

### 2. 配置副本集

```shell
# 使用 --replSet name 选项重启 server-1 实例
mongod --replSet spock -f mongod.conf --fork
# 再用上述命令启动两个 mongod 服务器作为副本集中的其他成员
# 创建一个配置文件：列出每一个成员，且将配置文件发送给 server-1 
# server-1 会负责将配置文件传播给其他成员
config = {
    "_id":"spock",
    "members":[
        {"_id":0,"host":"server-1:27017"}
        {"_id":1,"host":"server-2:27017"}
        {"_id":2,"host":"server-3:27017"}
    ]
}

# 使用 config 对象对副本集进行初始化
# 连接到 server-1
db = (new Mongo("seÿver-l:27017")).getDB("test")
# 初始化副本集
rs.initiate(config)
# 所有成员都配置完成之后，它们会自动选出一个主节点，然后就可以正常处理读写请求了
```

- **`rs` 辅助函数**： `rs` 是一个全局变量，其中包含与复制相关的辅助函数，这些函数大多只是数据库命令的包装器，可使用 `rs.help()` 查看

- **网络注意事项**： 副本集内的每个成员都必须通过正确的网络配置能够连接到其他所有成员

  > - 副本集的配置中不应该使用 `localhost` 作为主机名
  > - 如果所有副本集成员都运行在同一台机器上，那么 `localhost` 可以被正确解析；如果副本集是运行在多台机器上的，那么 `localhost` 就无法被解析为正确的主机名

### 3. 修改副本集配置

- `rs.add("server-4:27017")`： 为副本集添加新成员

- `rs.remove("server-1:27017")`： 从副本集中删除成员

  > - 删除成员时，会在 shell 中得到很多无法连接数据库的错误信息，这是正常的，这实际上说明配置修改成功了
  > - 重新配置副本集时，主节点需要先退化为普通的备份节点，以便接受新的配置，然后会恢复

- `rs.config()`： 査看配置修改是否成功
- `rs.reconfig(config)`： 将新的配置文件发送给数据库

### 4. 成员配置选项

- **选举仲裁者**： 唯一作用就是参与选举，为帮助具有两个成员的副本集能够满足“大多数”这个条件

  > - `rs.addArb()`： 将仲裁者添加到副本集中
  > - 最多只需要一个仲裁者，如果节点数量是奇数，就不需要仲裁者
  > - 仲裁者的目的： 避免出现平票

- **优先级**： 用于表示一个成员渴望成为主节点的程度，取值范围可以是0~100，默认为 1

  > - 被动成员： 优先级为 0 的成员永远不能够成为主节点
  > - 拥有最高优先级的成员会优先选举为主节

- 延迟备份节点： 可以使用 `slaveDelay` 设置一个延迟的备份节点，延迟备份节点的数据会比主节点延迟指定的时间

  > `slaveDelay` 要求成员的优先级是 0， 如果你的应用会将读请求路由到备份节点,应该将延迟备份节点隐藏掉，以免读请求被路由到延迟备份节点

- **备份节点的索引**： 备份节点可以没有索引，`buildIndexs:false` 可以阻止备份节点创建索引

## 2. 副本集的构成

### 1. 同步

- 复制用于在多台服务器之间备份数据

  > - MongoDB 的复制功能是使用操作日志 oplog实现的，操作日志包含了主节点的毎一次写操作
  >
  > - oplog 是主节点的 local 数据库中的一个固定集合
  >
  > - 备份节点通过査询这个集合就可以知道需要进行复制的操作

- 每个备份节点都维护着自己的 oplog，记录着每一次从主节点复制数据的操作

  > 复制操作的过程是先复制数据再写入 oplog，所以，备份节点可能会在已经同步过的数据上再次执行复制操作

- **初始化同步**： 副本集中的成员启动之后，就会检査自身状态，确定是否可以从某个成员那里进行
  同步，如果不行的话，会尝试从副本的另一个成员那里进行完整的数据复制

  > 跟踪初始化同步过程，最好的方式就是査看服务器日志
  >
  > 步骤：
  >
  > - 首先**准备工作**： ：选择一个成员作为同步源，在 `local.me` 中为自己创建一个标识符，删除所有已存在的数据库，以一个全新的状态开始进行同步
  >
  > - 然后是**克隆**： 将同步源的所有记录全部复制到本地
  >
  > - 然后**进入 oplog 同步的第一步**： 克隆过程中的所有操作都会被记录到 oplog 中
  >
  >   > 如果有文档在克隆过程中被移动而被遗漏，导致没有被克隆，则可能需要重新进行克隆
  >
  > - 接下来是 **oplog 同步的第二步**： 用于将第一个 oplog 同步中的操作记录下来
  >
  > - 当本地的数据与主节点在某个时间点的数据集完全一致时，可以**开始创建索引**
  >
  > - **oplog 同步的最后一步**： 将创建索引期间的所有操作全部同步过来，防止该成员成为备份节点
  >
  >   > - 若当前节点的数据仍然远远落后于同步源时，进行该步
  >
  > - 当前成员已经完成了初始化同步，切换到普通同步状态，这时当前成员就可以成为备份节点

- 陈旧数据： 如果备份节点远远落后于同步源当前的操作，那么这个备份节点就是陈旧的

  > - 当一个备份节点陈旧之后，它会查看副本集中的其他成员，如果某个成员的 oplog足够详尽，可以用于处理那些落下的操作，就从这个成员处进行同步
  > - 如果任何一个成员的 oplog 都没有参考价值，那么这个成员上的复制操作就会中止，这个成员需要重新进行完全同步

### 2. 心跳

- 为了维护集合的最新视图，每个成员每隔两秒钟就会向其他成员发送一个**心跳请求**

  > 心跳请求的信息量非常小，用于检査毎个成员的状态

- 心跳最重要的功能：让主节点知道自己是否满足集合“大多数”的条件

  > 如果主节点不再得到“大多数” 服务器的支持，它就会退位，变成备份节点

- **成员状态**： 各个成员会通过心跳将自己的当前状态告诉其他成员

  - 主节点

  - 备份节点

  - `STARTUP`： 成员刚启动时处于这个状态

    > 在这个状态下，MongoDB 会尝试加载成员的副本集配置，配置加载成功之后，就进入STARTUP2 状态

  - `STARTUP2`： 整个初始化同步过程都处于这个状态

    > - 如果在普通成员上，这个状态只会持续几秒钟
    > - 这个状态下，\MongoDB 会创建几个线程，用于处理复制和选举,然后就会切换到状态 RECOVERING

  - `RECOVERING`： 这个状态表明成员运转正常，但是暂时还不能处理读取请求

    > 如果有成员处于这个状态，可能会造成轻微的系统过载
    >
    > - 在启动过程中，成为备份节点之前，每个成员都要经历 RECOVERING 状态
    > - 在处理非常耗时的操作时，成员也可能进入 RECOVERING 状态
    > - 当一个成员与其他成员脱节时，也会进入 RECOVERING  状态

  - `ARBITER`： 在正常的操作中，仲裁者应该始终处于 ARBITER 状态

  系统出现问题时会处于下面这些状态：

  - `DOWN`： 如果一个正常运行的成员变得不可达，它就处于 DOWN 状态

  - `UNKNOWN`： 如果一个成员无法到达其他任何成员，其他成员就无法知道它处干什么状态，会将其报告为 UNKNOWN 状态

    > 表明：
    >
    > - 这个未知状态的成员挂掉了
    > - 或两个成员之间存在网络访问问题

  - `REMOVED`： 当成员被移出副本集时，它就处于这个状态

    > 如果被移出的成员又被重新添加到副本集中，它就会回到“正常” 状态

  - `ROLLBACK`： 如果成员正在进行数据回滚，它就处于 ROLLBACK 状态

    > 回滚过程结束时，服务器会转换为 RECOVERING 状态，然后成为备份节点

  - `FATAL`： 如果一个成员发生了不可挽回的错误，也不再尝试恢复正常的话，它就处于 FATAL 状态

    > 通常应该重启服务器，进行重新同步或者是从备份中恢复

### 3. 选举

- 当一个成员无法到达主节点时，它就会申请被选举为主节点

- 假如没有反对的理由，其他成员就会对这个成员进行选举投票

  > - 如果这个成员得到副本集中“大多数” 赞成票，它就选举成功，会转换到主节点状态
  >
  > - 如果达不到“大多数”的要求，那么选举失败，它仍然处于备份节点状态

### 4. 回滚

- 如果要回滚的数据量大于300MB，或者要回滚3 0 分钟以上的操作，回滚就会失败
- 对于回滚失败的节点，必须要重新同步
- 失败原因： 是备份节点远远落后于主节点，而这时主节点却挂了
- 为了保证成员回滚成功，最好的方式是保持备份节点的数据尽可能最新

## 3. 连接副本集

### 1. 客户端到副本集的连接

- 默认情况下，驱动程序会连接到主节点，并且将所有请求都路由到主节点

  > 应用程序可以像使用单台服务器一样进行读和写，副本集会在后台默默处理热备份

### 2. 自定义复制保证规则

- 副本集允许创建自己的规则，并且可以传递给 `getLastError` 以保证写操作被复制到所需的服务器上

- 保证复制到每个数据中心的一台服务器上： 这样，万一某个数据中心掉线了，其他每一个数据中心都有一份最新的本地数据副本
- 创建自定义复制规则的两个步骤：
  - 使用键值对设置成员的"tags" 字段
  - 基于刚刚创建的分组创建规则
- 对一致性要求非常高的应用程序不应该从备份节点读取数据
- 将读请求发送给备份节点，以便实现分布式负载

## 4. 管理副本集

- 以单机模式启动服务器： 指要重启成员服务器，让它成为一个单机运行的服务器，而不再是一个副本集成员



# 四、分片

## 1. 分片简介

- **分片(分区)**： 是指将数据拆分，将其分散存放在不同的机器上的过程
- MongoDB 支持自动分片，可以使数据库架构对应用程序不可见，也可以简化系统管理
- 分片的目标之一： 是创建一个拥有多台机器的集群，整个集群对应用程序来说就像是一台单机服务器
- 分片用来：
  - 增加可用 RAM
  - 增加用磁盘空间
  - 减轻单台服务器的负载
  - 处理单个 mongod 无法承受的吞吐量

## 2. 分片配置

- 创建集群的第一步是启动所有所需 mongos 进程

  > - 可启动任意数量的mongos 进程
  > - 每个 mongos 进程必须按照列表顺序，使用相同的配置服务器列表

- 配置服务器保存着集群和分片的元数据，即各分片包含哪些数据的信息，保存的只是数据的分布表

- 可通过增加分片来增加集群容量

- MongoDB 不会自动对数据进行拆分

- 每个 mongos 都必须能够根据给定的片键找到文档的存放位置

- 可使用块包含的文档范围来描述块

- mongos 会记录在毎个块中插入了多少数据，一旦达到某个阈值，就会检査是否需要对块进行拆分

- 均衡器： 负责数据的迁移，它会周期性地检査分片间是否存在不均衡，如果存在，则会开始块的迁移

## 3. 选择片键

- 片键： 对集合进行分片时，要选择一或两个字段用于拆分数据

  > 一旦拥有多个分片，再修改片键几乎是不可能的事情

### 1. 数据分发

- 拆分数据最常用的数据分发方式：

  - **升序片键**： 是一种会随着时间稳定增长的字段

  - **随机分发的片键**： 随机分发的键可以是用户名、邮件地址、UDID(唯一设备标识符）、MD5 散列值，或数据集中其他一些**没有规律的健**

  - **基于位置的片键**： 可以是用户的 IP、经纬度、地址

    > 所有与该键值比较接近的文档都会被保存在同一范围的块中

### 2. 片键策略

- **散列片键**： 追求数据加载速度的极致

  > - 散列片键可使其他任何键随机分发
  > - 若打算在大量査询中使用升序键，但同时又希望写入数据随机分发的话，散列片键会是个很好选择
  > - 弊端： 是无法使用散列片键做指定目标的范围查询

- `GridFS` 的散列片键： 

  > - GridFS 会将文件拆分为块，而分片也会将集合拆分为块
  > - GridFS 集合适合做分片，因为包含大量的文件数据

- 流水策略： 让集群中强大的服务器处理更多的负载

  > 弊端： 是需做一些修改才能进行扩展

- **多热点**： 创建多个热点，写请求会均衡地分布在集群内，而在单个分片上则是以升序分布的

  > - 写请求分布在集群中时，分片是最高效的
  > - 复合片键：
  >   - 第一个值只是比较粗略的随机值，势也比较低
  >   - 第二个值是个升序键

### 3. 片键规则和指导方针

- **片键限制**： 
  - 片键不可以是数组
  - 文档一旦插入，其片键值就无法修改了

- **片键的势**： 分片在势比较髙的字段上性能更佳

### 4. 控制数据分发

- **对多个数据库和集合使用一个集群**： MongoDB 集合均衡地分发到集群中的分片上，如果保存的数据比较均匀，则该方法非常有效
- **手动分片**： 使用命令 `moveChunk` 手动对数据进行迁移

## 4. 分片管理

### 1. 检查集群状态

- `sh.status()`： 可査看分片、数据库和分片集合的摘要信息

  > - 如果块的数量较少，则该命令会打印出每个块的保存位置
  > - 否则它只会简单地给出集合的片键，以及每个分片的块数

- **检查配置信息**： 集群相关的所有配置信息都保存在配置服务器上 `config` 数据库的集合中

  > - 永远不要直接连接到配置服务器，以防配置服务器数据被不小心修改或删除
  > - 应先连接到 mongos，然后通过数据库来査询相关信息： `use config`

  - `config.shards`：`shards` 集合跟踪记录集群内所有分片的信息
  - `config.databases`：`databases` 集合跟踪记录集群中所有数据库的信息，不管数据库有没有被分片
  - `config.collections`： `collections` 集合跟踪记录所有分片集合的信息（非分片集合信息除外）
  - `config.tags`：该集合的创建是在为系统配置分片标签时发生的
  - `config.settings`：该集合含有当前的均衡器设置和块大小的文档信息

### 2. 查看网络连接

- **查看连接统计**： 可使用命令 `connPoolStats` 査看 `mongos 和 mongod`之间的连接信息，并可得知服务器上打开的所有连接

- **限制连接数量**： 在命令行配置中使用 `maxConns` 选项，可以限制 mongos 能够创建的连接数量

  > **计算公式**：`maxConns = 2000 - (mongos 进程数量 * 3) - (每个副本集的成员数量 * 3) - (其他/mongos进程数量)`
  >
  > - `(mongos进程的数量 * 3)`： 每个mongos 会为每个mongod 创建3 个连接：
  >   - 一个用于转发客户端请求
  >   - 一个用于追踪错误信息，即写回监听器
  >   - 一个用于监控副本集状态
  > - `每个副本集的成员数量 * 3)`： 主节点会与每个备份节点创建一个连接，而每个备份节点会与主节点创建两个连接，因此总共是3 个连接
  > - `(其他/mongos进程的数量)`： `其他 `指其他可能连接到 mongod 的进程数量，这种连接包括 MMS 代理、shell 的直接连接（管理员用）， 或迁移时连接到其他分片的连接

### 3. 服务器管理

- 随着集群的增长，我们可能需要增加集群容量或者是修改集群配置

- **添加服务器**： 可随时向集群中添加新的 mongos，只要保证在 mongos 的 `--configdb` 选项中指定了一组正确的配置服务，mongos 即可立即与客户端建立连接

- **修改分片的服务器**： 要修改分片的成员，需直接连接到分片的主服务器上，然后对副本集进行重新配置

  > - 集群配置会自动检测更改，并将其更新上，不要手动修改 `config.shards`
  >
  > - 只有在使用单机服务器作为分片，而不是使用副本集作为分片时，才需手动修改`config.shards`
  > - 将单机服务器分片修改为副本集分片：
  >   - 停止向系统发送请求
  >   - 关闭单机服务器`server-1`和所有的 mongos 进程
  >   - 以副本集模式重启服务器`server-1`
  >   - 连接到服务器`server-1`，将其作为一个单成员副本集进行初始化
  >   - 连接到配置服务器，替换该分片的入口，在 `config.shards` 中将分片名称替换为
  >     `setName/server-1:27017`的形式，确保三个配置服务器都拥有相同的配置信息
  >   - 重启所有 mongos 进程，它们会在启动时从配置服务器读取分片数据，然后将副本集当作分片对待
  >   - 重启所有分片的主服务器，刷新其配置数据
  >   - 再次向系统发送请求
  >   - 向服务器`server-1`副本集中添加新成员

- **删除分片**： 
  - 首先保证均衡器是开启的：
    -  在排出数据的过程中，均衡器会负责将待删除分片的数据迁移至其他分片
    - 执行 `removeShard`命令，开始排出数据
    - `removeShard` 将待刪除分片的名称作为参数，然后将该分片上的所有块都移至其他分片上
- **修改配置服务器**： 要修改配置服务器，首先必须关闭所有的 mongos 进程

### 4. 数据均衡

- 均衡器： 

  > - 在执行几乎所有的数据库管理操作之前，都应先关闭均衡器，shell 命令： `sh.setBalancerState(false)`
  > - `config.locks`： 可查看均衡过程是否仍在进行
  > - **均衡过程会增加系统负载**：目标分片必须査询源分片块中的所有文档，将文档插入目标分片的块中，源分片最后必须删除这些文档
  > - 迁移会导致性能问题：
  >   - 使用热点片键可保证定期迁移，系统必须有能力处理源源不断写入到热点分片上的数据
  >   - 向集群中添加新的分片时，均衡器会试图为该分片写入数据，从而触发一系列的迁移过程

- **修改块大小**： 块的大小默认为 64MB，可通过减小块的大小，提高迁移速度

  > `config.settings`： 用于修改块的大小，如：修改块的大小为 32MB：`db.settings.save({"_id":"chunksize","value":32})`

- **移动块(迁移)**： 同一块内的所有数据都位于同一分片上，如该分片的块数量比其他分片多，则 MongoDB 会将其中的一部分块移至其他块数量较少的分片上

  > - `moveChunk` 函数用于手动移动块

- **特大块**： 不可拆分和移动的块，块的大小超出了 `config.settings` 中设置的最大块大小

  > - **分发特大块**： 为修复由特大块引发的集群不均衡，将特大块均衡地分发到其他分片上
  >
  > - **防止出现特大块**： 为防止特大块的出现，可修改片键，细化片键的粒度
  >
  >   > 应尽可能保证毎个文档都拥有唯一的片键值，或至少不要出现某个片键值的数据块超出最大块大小设定值的情况

- **刷新配置**： mongos 有时无法从配置服务器正确更新配置，可使用命令 `flushRouterConfig` 手动刷新所有缓存

# 五、应用管理

## 1. 了解应用的动态

### 1. 了解正在进行的操作

- 函数 `db.currentOp()`： 査看正在进行的操作

  > 该函数会列出数据库正在进行的所有操作，输出的信息中有些重要的字段
  >
  > - `opid`： 操作的唯一标识符，可通过它来终止一个操作
  > - `active`： 表示该操作是否正在运行
  > - `secs_running`： 表示该操作已经执行的时间
  > - `op`： 表示操作的类型，通常是査询、插入、更新、删除中的一种
  > - `desc`： 该值可与日志信息联系起来
  > - `locks`： 描述该操作使用的锁的类型
  > - `waitingForLock`： 表示该操作是否因正在等待其他操作交出锁而处于阻塞状态
  > - `numYields`： 表示该操作交出锁，而使其他操作得以运行的次数
  > - `lockstats.timeAcquiringMiros`： 表示该操作需要多长时间才能取得所需的锁

- `db.currentOp()`最常见的作用： 就是用来寻找速度较慢的操作

- `db.killOp(param)`： 将该操作的 `opid` 作为参数可终止操作的进行

  > - 只有交出了锁的进程才能被终止

- **避免幽灵操作**： 阻止幽灵写入的最好方式是**使用应答式写入**，即毎次写入操作都会等待上一次写入操作完成后才会进行下去，而非在上一次写入进入数据库服务器的缓存区就开始下一次写入

### 2. 使用系统分析器

- 可利用系统分析器来查找耗时过长的操作
- 系统分析器可记录特殊集合 `system.profile` 中的操作，并提供大量有关耗时过长的操作信息
- 默认情况下，系统分析器处于关闭状态，不会进行任何记录，`db.setProfilingLevel()` 可开启分析器

### 3. 计算空间消耗

- **文档**： `Object.bsonsize()` 函数返回该文档存储在 MongoDB 中时占用的空间大小
- **集合**： `stats` 函数可用来显示一个集合的信息

### 4. 使用 mongotop 和 mongostat

- `mongotop ` 类似于 UNIX 中的 `top` 工具，可概述哪个集合最为繁忙,可通过运行 `mongotop-locks` 得知毎个数据库的锁状态
- `mongostat` 提供有关服务器的信息，默认每秒输出一次包含当前状态的列表，可在命令行中传入参数更改时间间隔

## 2. 数据管理

### 1. 配置身份验证

- 在以任务为颗粒的粗粒度访问方式中，MongoDB 支持针对单个连接进行身份验证

- **身份验证基本原理**：`admin(管理员)和local(本地)` 是两个特殊的数据库，它们当中的用户可对任何数据库进行操作

- **配置身份验证**： 启用身份验证后，客户端必须登录才能进行读写

- **身份验证的工作原理**： 数据库中的用户是作为文档被储存在其 `syste.users` 集合中，这种用以保存用户信息的文档结构是 `{user:username,readonly:true,pwd:password hash}`

  > `password hash` 是基干username 和密码生成的散列值

### 2. 建立和删除索引

- **在独立的服务器上建立索引**：在独立的服务器上，可在空闲时间于后台建立索引

  > 后台建立索引： `db.foo.ensureIndex({"somefield":1},{"background":true})`

- **在副本集上建立索引**： 较小集合中，在主节点中建立索引，然后等待其被复制到其他备份节点

  > 对于较大的集合，推荐采用的方式是：
  >
  > - 关闭一个备份节点
  > - 将其作为独立的节点启动
  > - 在这一服务器上建立索引
  > - 重新将其作为成员加入副本集
  > - 对每个备份节点执行同样的操作

- **在分片集群上建立索引**： 在分片集群上建立索引，与在副本集中建立索引的步骤相同，不过需要在毎个分片上分别建立一次

- **删除索引**： 可使用 `dropIndexes` 命令并指定索引名来删除索引

  > 查询 `system.indexes` 集合可找出索引名

### 3. 预热数据

- 重启机器或启动一台新的服务器，会耗费一段时间供 MongoDB 将所有所需数据从磁盘载入内存
- 有几种方式可在服务器正式上线之前将数据载入内存，以避免在应用运行时的麻烦：
  - **将数据库移至内存**
  - **将集合移至内存**

- **自定义预热**： 
  - 加载一个特定的索引
  - 加载最近更新的文档
  - 加载最近创建的文档
  - 重放应用使用记录

### 4. 压缩数据

- `compact` 命令： 为消除空区段，并髙效重整集合，如： `db.runCommand({"compact":"collName"})`
- 压缩操作会消耗大量资源，不应在 MongoDB 向客户端提供服务时计划压缩操作
- 压缩操作会将文档尽可能地安排在一起，文档间的间隔参数默认为 1

### 5. 移动集合

- `renameCollection` 命令可重命名集合
- 要想在数据库间移动集合，必须进行转储和恢复操作，或手动复制集合中的文档
- `cloneCollection` 命令可移动集合

## 3. 持久性

- 持久性： 是操作被提交后可持久保存在数据库中的保证

### 1. 日记系统的用途

- MongoDB 会在进行写入时建立一条日志，日记中包含了此次写入操作具体更改的磁盘地址和字节

- 一旦服务器突然停机，可在启动时对日记进行重放，从而重新执行那些停机前没能够刷新到磁盘的写入操作

- 数据文件默认每 60 秒刷新到磁盘一次

- **批量提交写入操作**：MongoDB 默认每隔 100 毫秒，或是写入数据达到若干兆字节时，便会将这些操作写入日记

  > - MongoDB 会成批量地提交更改，即每次写入不会立即刷新到磁盘

- **设定提交时间间隔**： 运行 `setParameter` 命令，设定 `journalCommitInterval` 的值，可调整两次提交间的时间间隔，减少日记被干扰几率

### 2. 关闭日记系统

- 日记系统会影响 MongoDB 的写入速度，如果写入数据的价值不及写入速度降低带来的损失，可能会想禁用日记系统
- 禁用日记系统的**缺陷**： MongoDB 无法保证发生崩溃后数据的完整性

数据库在崩溃后能够继续工作的做法：

- 替换数据文件
- 修复数据文件

### 3. 检验数据损坏

- 命令 `validate`： 可用于检验集合是否有损坏