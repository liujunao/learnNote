> å®Œæ•´çŸ¥è¯†æ¨èï¼šhttps://www.huaxiaozhuan.com/
>
> transformer æ¨èï¼šhttps://transformers.run/

---

https://easyai.tech/ai-definition/transformer/

- transfomer å·¥å…·åŒ…ï¼šhttps://huggingface.co/docs/transformers/main/zh/index

- transformer æ¨¡å‹åŠåº”ç”¨ï¼šhttps://transformers.run/

---

# ä¸€ã€ç®€ä»‹

### 1.1 åŸºæœ¬æ¦‚å¿µ

- Transformer æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¤§éƒ½é‡‡ç”¨è‡ªç›‘ç£å­¦ä¹  (Self-supervised learning) çš„æ–¹å¼åœ¨å¤§é‡ç”Ÿè¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒ

    > è‡ªç›‘ç£å­¦ä¹ æ˜¯ä¸€ç§è®­ç»ƒç›®æ ‡å¯ä»¥æ ¹æ®æ¨¡å‹çš„è¾“å…¥è‡ªåŠ¨è®¡ç®—çš„è®­ç»ƒæ–¹æ³•

- ä¸¤ä¸ªå¸¸ç”¨çš„é¢„è®­ç»ƒä»»åŠ¡ï¼š

    - **å› æœè¯­è¨€å»ºæ¨¡**ï¼šåŸºäºå¥å­çš„å‰ ğ‘› ä¸ªè¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œè¾“å‡ºä¾èµ–äºè¿‡å»å’Œå½“å‰çš„è¾“å…¥

        > â€œå› æœè¯­è¨€å»ºæ¨¡â€æ˜¯ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼Œåªä½¿ç”¨å‰é¢çš„è¯æ¥é¢„æµ‹å½“å‰è¯ï¼Œç”± NNLM é¦–æ¬¡è¿ç”¨

    - **é®ç›–è¯­è¨€å»ºæ¨¡**ï¼šåŸºäºä¸Šä¸‹æ–‡ï¼ˆå‘¨å›´çš„è¯è¯­ï¼‰æ¥é¢„æµ‹å¥å­ä¸­è¢«é®ç›–æ‰çš„è¯è¯­ (masked word)

        > â€œé®ç›–è¯­è¨€å»ºæ¨¡â€æ˜¯ Word2Vec æ¨¡å‹æå‡ºçš„ CBOW

- **è®­ç»ƒæ–¹å¼**ï¼š

    - **é¢„è®­ç»ƒ**ï¼šä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹
    - **è¿ç§»å­¦ä¹ **ï¼šç”¨è‡ªå·±çš„ä»»åŠ¡è¯­æ–™å¯¹æ¨¡å‹è¿›è¡Œâ€œäºŒæ¬¡è®­ç»ƒâ€ï¼Œé€šè¿‡å¾®è°ƒå‚æ•°ä½¿æ¨¡å‹é€‚ç”¨äºæ–°ä»»åŠ¡

### 1.2 ç»“æ„

#### (1) Encoder ä¸ Decoder

æ ‡å‡†çš„ Transformer æ¨¡å‹ä¸»è¦ç”±ä¸¤ä¸ªæ¨¡å—æ„æˆï¼š

- **Encoderï¼ˆå·¦è¾¹ï¼‰ï¼š**è´Ÿè´£ç†è§£è¾“å…¥æ–‡æœ¬ï¼Œä¸ºæ¯ä¸ªè¾“å…¥æ„é€ å¯¹åº”çš„è¯­ä¹‰è¡¨ç¤ºï¼ˆè¯­ä¹‰ç‰¹å¾ï¼‰
- **Decoderï¼ˆå³è¾¹ï¼‰ï¼š**è´Ÿè´£ç”Ÿæˆè¾“å‡ºï¼Œä½¿ç”¨ Encoder è¾“å‡ºçš„è¯­ä¹‰è¡¨ç¤ºç»“åˆå…¶ä»–è¾“å…¥æ¥ç”Ÿæˆç›®æ ‡åºåˆ—

è¿™ä¸¤ä¸ªæ¨¡å—å¯ä»¥æ ¹æ®ä»»åŠ¡çš„éœ€æ±‚è€Œå•ç‹¬ä½¿ç”¨ï¼š

- **çº¯ Encoder æ¨¡å‹ï¼š**é€‚ç”¨äºåªéœ€è¦ç†è§£è¾“å…¥è¯­ä¹‰çš„ä»»åŠ¡ï¼Œä¾‹å¦‚å¥å­åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ï¼›
- **çº¯ Decoder æ¨¡å‹ï¼š**é€‚ç”¨äºç”Ÿæˆå¼ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆï¼›
- **Encoder-Decoder æ¨¡å‹**æˆ– **Seq2Seq æ¨¡å‹ï¼š**é€‚ç”¨äºéœ€è¦åŸºäºè¾“å…¥çš„ç”Ÿæˆå¼ä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘ã€æ‘˜è¦

<img src="../../pics/turn_transformer/turn_1.png" width=400 align=left>

#### (2) Transformer æ¨¡å‹ç»“æ„

Transformer æ¨¡å‹æœ¬æ¥æ˜¯ä¸ºäº†ç¿»è¯‘ä»»åŠ¡è€Œè®¾è®¡ï¼š

- åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒEncoder æ¥å—æºè¯­è¨€çš„å¥å­ä½œä¸ºè¾“å…¥ï¼Œè€Œ Decoder åˆ™æ¥å—ç›®æ ‡è¯­è¨€çš„ç¿»è¯‘ä½œä¸ºè¾“å…¥

- åœ¨ Encoder ä¸­ï¼Œç”±äºç¿»è¯‘ä¸€ä¸ªè¯è¯­éœ€è¦ä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå› æ­¤æ³¨æ„åŠ›å±‚å¯ä»¥è®¿é—®å¥å­ä¸­çš„æ‰€æœ‰è¯è¯­ï¼›

    è€Œ Decoder æ˜¯é¡ºåºåœ°è¿›è¡Œè§£ç ï¼Œåœ¨ç”Ÿæˆæ¯ä¸ªè¯è¯­æ—¶ï¼Œæ³¨æ„åŠ›å±‚åªèƒ½è®¿é—®å‰é¢å·²ç»ç”Ÿæˆçš„å•è¯ã€‚

    > æ³¨æ„åŠ›å±‚çš„ä½œç”¨å°±æ˜¯è®©æ¨¡å‹åœ¨å¤„ç†æ–‡æœ¬æ—¶ï¼Œå°†æ³¨æ„åŠ›åªæ”¾åœ¨æŸäº›è¯è¯­ä¸Š
    >
    > ä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œä¼šå°†æ•´ä¸ªç›®æ ‡åºåˆ—éƒ½é€å…¥ Decoderï¼Œç„¶ååœ¨æ³¨æ„åŠ›å±‚ä¸­é€šè¿‡ Mask é®ç›–æ‰æœªæ¥çš„è¯è¯­æ¥é˜²æ­¢ä¿¡æ¯æ³„éœ²

**åŸå§‹çš„ Transformer æ¨¡å‹ç»“æ„**ï¼š

- Decoder ä¸­çš„ç¬¬ä¸€ä¸ªæ³¨æ„åŠ›å±‚å…³æ³¨ Decoder è¿‡å»æ‰€æœ‰çš„è¾“å…¥ï¼Œè€Œç¬¬äºŒä¸ªæ³¨æ„åŠ›å±‚åˆ™æ˜¯ä½¿ç”¨ Encoder çš„è¾“å‡ºï¼Œå› æ­¤ Decoder å¯ä»¥åŸºäºæ•´ä¸ªè¾“å…¥å¥å­æ¥é¢„æµ‹å½“å‰è¯è¯­

- åœ¨ Encoder/Decoder çš„æ³¨æ„åŠ›å±‚ä¸­ï¼Œä¼šä½¿ç”¨ Attention Mask é®ç›–æ‰æŸäº›è¯è¯­æ¥é˜²æ­¢æ¨¡å‹å…³æ³¨å®ƒä»¬

    > ä¾‹å¦‚ä¸ºäº†å°†æ•°æ®å¤„ç†ä¸ºç›¸åŒé•¿åº¦è€Œå‘åºåˆ—ä¸­æ·»åŠ çš„å¡«å…… (padding) å­—ç¬¦ã€‚

<img src="../../pics/turn_transformer/turn_2.png" width=600 align=left>

#### (3) Transformer å®¶æ—

> å‚è€ƒï¼šhttps://transformers.run/c1/transformer/#transformer-%E5%AE%B6%E6%97%8F

<img src="../../pics/turn_transformer/turn_3.png" width=600 align=left>

# äºŒã€æ³¨æ„åŠ›æœºåˆ¶

> Transformer æ¨¡å‹ä¹‹æ‰€ä»¥å¦‚æ­¤å¼ºå¤§ï¼Œæ˜¯å› ä¸ºæŠ›å¼ƒäº†å¾ªç¯ç½‘ç»œå’Œå·ç§¯ç½‘ç»œï¼Œè€Œé‡‡ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶ (Attention) æ¥å»ºæ¨¡æ–‡æœ¬

### 2.1 RNN ä¸ CNN ç¼ºç‚¹

NLP ç¥ç»ç½‘ç»œæ¨¡å‹çš„æœ¬è´¨å°±æ˜¯å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¯¹ token åºåˆ— ğ‘‹ çš„å¸¸è§„ç¼–ç æ–¹å¼ï¼š

> å¸¸è§„çš„åšæ³•æ˜¯é¦–å…ˆå¯¹å¥å­è¿›è¡Œåˆ†è¯ï¼Œç„¶åå°†æ¯ä¸ªè¯è¯­ (token) éƒ½è½¬åŒ–ä¸ºå¯¹åº”çš„è¯å‘é‡ (token embeddings)ï¼Œè¿™æ ·æ–‡æœ¬å°±è½¬æ¢ä¸ºä¸€ä¸ªç”±è¯è¯­å‘é‡ç»„æˆçš„çŸ©é˜µ X

- **RNN æ–¹æ¡ˆ**ï¼šæ¯ä¸€ä¸ªè¯è¯­ ğ‘¥ğ‘¡ å¯¹åº”çš„ç¼–ç ç»“æœ ğ‘¦ğ‘¡ é€šè¿‡é€’å½’åœ°è®¡ç®—å¾—åˆ°
    $$
    y_t = f(y_{t-1}, xt)
    $$

    > RNN çš„é€’å½’ç»“æ„å¯¼è‡´å…¶æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œå› æ­¤é€Ÿåº¦è¾ƒæ…¢ã€‚è€Œä¸” RNN æœ¬è´¨æ˜¯ä¸€ä¸ªé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼Œéš¾ä»¥å­¦ä¹ åˆ°å…¨å±€çš„ç»“æ„ä¿¡æ¯

- **CNN æ–¹æ¡ˆ**ï¼šé€šè¿‡æ»‘åŠ¨çª—å£åŸºäºå±€éƒ¨ä¸Šä¸‹æ–‡æ¥ç¼–ç æ–‡æœ¬ï¼Œä¾‹å¦‚æ ¸å°ºå¯¸ä¸º 3 çš„å·ç§¯æ“ä½œå°±æ˜¯ä½¿ç”¨æ¯ä¸€ä¸ªè¯è‡ªèº«ä»¥åŠå‰ä¸€ä¸ªå’Œåä¸€ä¸ªè¯æ¥ç”ŸæˆåµŒå…¥å¼è¡¨ç¤º
    $$
    y_t = f(x_{t-1}, x_t, x_{t+1})
    $$

    > CNN èƒ½å¤Ÿå¹¶è¡Œåœ°è®¡ç®—ï¼Œå› æ­¤é€Ÿåº¦å¾ˆå¿«ï¼Œä½†æ˜¯ç”±äºæ˜¯é€šè¿‡çª—å£æ¥è¿›è¡Œç¼–ç ï¼Œæ‰€ä»¥æ›´ä¾§é‡äºæ•è·å±€éƒ¨ä¿¡æ¯ï¼Œéš¾ä»¥å»ºæ¨¡é•¿è·ç¦»çš„è¯­ä¹‰ä¾èµ–

- **Attention æœºåˆ¶ç¼–ç æ•´ä¸ªæ–‡æœ¬**ï¼šç›¸æ¯” RNN è¦é€æ­¥é€’å½’æ‰èƒ½è·å¾—å…¨å±€ä¿¡æ¯ï¼ˆå› æ­¤ä¸€èˆ¬ä½¿ç”¨åŒå‘ RNNï¼‰ï¼Œè€Œ CNN å®é™…åªèƒ½è·å–å±€éƒ¨ä¿¡æ¯ï¼Œéœ€è¦é€šè¿‡å±‚å æ¥å¢å¤§æ„Ÿå—é‡ï¼ŒAttention æœºåˆ¶ä¸€æ­¥åˆ°ä½è·å–äº†å…¨å±€ä¿¡æ¯
    $$
    y_t = f(x_t, A, B)
    $$

    > ğ´,ğµ æ˜¯å¦å¤–çš„è¯è¯­åºåˆ—(çŸ©é˜µ)ï¼Œè‹¥å– ğ´=ğµ=ğ‘‹ å°±ç§°ä¸º Self-Attentionï¼Œå³ç›´æ¥å°† ğ‘¥ğ‘¡ ä¸è‡ªèº«åºåˆ—ä¸­çš„æ¯ä¸ªè¯è¯­æ¯”è¾ƒï¼Œæœ€åç®—å‡º ğ‘¦ğ‘¡

### 2.2 Attention å®ç°

#### (1) Scaled Dot-product Attention

Scaled Dot-product Attention å…±åŒ…å« 2 ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. **è®¡ç®—æ³¨æ„åŠ›æƒé‡**ï¼šä½¿ç”¨æŸç§ç›¸ä¼¼åº¦å‡½æ•°åº¦é‡æ¯ä¸€ä¸ª query å‘é‡å’Œæ‰€æœ‰ key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦

    å¯¹äºé•¿åº¦ä¸º ğ‘š çš„ Query åºåˆ—å’Œé•¿åº¦ä¸º ğ‘› çš„ Key åºåˆ—ï¼Œè¯¥æ­¥éª¤ä¼šç”Ÿæˆä¸€ä¸ªå°ºå¯¸ä¸º ğ‘šÃ—ğ‘› çš„æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ

    > è¡¥å……ï¼šScaled Dot-product Attention ä½¿ç”¨ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦å‡½æ•°ï¼Œç”±äºç‚¹ç§¯å¯ä»¥äº§ç”Ÿä»»æ„å¤§çš„æ•°å­—ï¼Œä¼šç ´åè®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼Œå› æ­¤æ³¨æ„åŠ›åˆ†æ•°è¿˜éœ€è¦ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­æ¥æ ‡å‡†åŒ–å®ƒä»¬çš„æ–¹å·®ï¼Œç„¶åç”¨ä¸€ä¸ª softmax æ ‡å‡†åŒ–
    >
    > è¿™æ ·å°±å¾—åˆ°äº†æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ ğ‘¤ğ‘–ğ‘—ï¼Œè¡¨ç¤ºç¬¬ ğ‘– ä¸ª query å‘é‡ä¸ç¬¬ ğ‘— ä¸ª key å‘é‡ä¹‹é—´çš„å…³è”ç¨‹åº¦

2. **æ›´æ–° token embeddingsï¼š**å°†æƒé‡ ğ‘¤ğ‘–ğ‘— ä¸å¯¹åº”çš„ value å‘é‡ ğ‘£1,â€¦,ğ‘£ğ‘› ç›¸ä¹˜ä»¥è·å¾—ç¬¬ ğ‘– ä¸ª query å‘é‡æ›´æ–°åçš„è¯­ä¹‰è¡¨ç¤º $ğ‘¥_ğ‘–^â€²=âˆ‘_ğ‘—ğ‘¤_{ğ‘–ğ‘—}ğ‘£_ğ‘—$  

    > å½¢å¼åŒ–è¡¨ç¤ºä¸º $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt d_k})V$ï¼Œå…¶ä¸­ Qï¼ŒKï¼ŒV åˆ†åˆ«æ˜¯ queryã€keyã€value å‘é‡åºåˆ—
    >
    > å°†ä¸Šé¢çš„å…¬å¼æ‹†å¼€æ¥çœ‹æ›´åŠ æ¸…æ¥šï¼š$Attention(q_t,K,V) = \sum^m_{s=1}\frac{1}{Z}exp(\frac{<q_t, k_s>}{\sqrt d_k})v_s$
    >
    > æ•´ä¸ªæµç¨‹ï¼šé€šè¿‡ ğ‘ğ‘¡ è¿™ä¸ª query ä¸å„ä¸ª ğ‘˜ğ‘  å†…ç§¯å¹¶ softmax çš„æ–¹å¼æ¥å¾—åˆ° ğ‘ğ‘¡ ä¸å„ä¸ª ğ‘£ğ‘  çš„ç›¸ä¼¼åº¦ï¼Œç„¶ååŠ æƒæ±‚å’Œï¼Œå¾—åˆ°ä¸€ä¸ª ğ‘‘ğ‘£ ç»´çš„å‘é‡ï¼Œå…¶ä¸­å› å­ ğ‘‘ğ‘˜ èµ·åˆ°è°ƒèŠ‚ä½œç”¨ï¼Œä½¿å¾—å†…ç§¯ä¸è‡³äºå¤ªå¤§

<img src="../../pics/turn_transformer/turn_4.png" width=400 align=left>

```python
#ä»£ç åç»­è¡¥å……ï¼šhttps://transformers.run/c1/attention/#scaled-dot-product-attention
#é¦–å…ˆéœ€è¦å°†æ–‡æœ¬åˆ†è¯ä¸ºè¯è¯­ (token) åºåˆ—ï¼Œç„¶åå°†æ¯ä¸€ä¸ªè¯è¯­è½¬æ¢ä¸ºå¯¹åº”çš„è¯å‘é‡ (embedding)
#Pytorch æä¾›äº† torch.nn.Embedding å±‚æ¥å®Œæˆè¯¥æ“ä½œï¼Œå³æ„å»ºä¸€ä¸ªä» token ID åˆ° token embedding çš„æ˜ å°„è¡¨



#æ¥ä¸‹æ¥å°±æ˜¯åˆ›å»º queryã€keyã€value å‘é‡åºåˆ— Q, K, Vï¼Œå¹¶ä¸”ä½¿ç”¨ç‚¹ç§¯ä½œä¸ºç›¸ä¼¼åº¦å‡½æ•°æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°


#æœ€åå°†æ³¨æ„åŠ›æƒé‡ä¸ value åºåˆ—ç›¸ä¹˜



```

#### (2) Multi-head Attention

> æ³¨æ„ï¼šå½“ ğ‘„ å’Œ ğ¾ åºåˆ—ç›¸åŒæ—¶ï¼ŒScaled Dot-product Attention æ³¨æ„åŠ›æœºåˆ¶ä¼šä¸ºä¸Šä¸‹æ–‡ä¸­çš„ç›¸åŒå•è¯åˆ†é…éå¸¸å¤§çš„åˆ†æ•°ï¼ˆç‚¹ç§¯ä¸º 1ï¼‰ï¼Œè€Œåœ¨å®è·µä¸­ï¼Œç›¸å…³è¯å¾€å¾€æ¯”ç›¸åŒè¯æ›´é‡è¦

Multi-head Attentionï¼šæ¯ä¸ªæ³¨æ„åŠ›å¤´è´Ÿè´£å…³æ³¨æŸä¸€æ–¹é¢çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå¤šä¸ªå¤´å°±å¯ä»¥è®©æ¨¡å‹åŒæ—¶å…³æ³¨å¤šä¸ªæ–¹é¢

- é¦–å…ˆé€šè¿‡çº¿æ€§æ˜ å°„å°† ğ‘„,ğ¾,ğ‘‰ åºåˆ—æ˜ å°„åˆ°ç‰¹å¾ç©ºé—´ï¼Œæ¯ä¸€ç»„çº¿æ€§æŠ•å½±åçš„å‘é‡è¡¨ç¤ºç§°ä¸ºä¸€ä¸ªå¤´ (head)
- ç„¶ååœ¨æ¯ç»„æ˜ å°„åçš„åºåˆ—ä¸Šå†åº”ç”¨ Scaled Dot-product Attention

---

**å…¬å¼**ï¼š
$$
head_i = Attention(QW^Q_i, KW^K_i, VW^V_i) \\
MultiHead(Q, K, V) = Concat(head_1,...,head_n)
$$

- å…¶ä¸­ï¼ŒW æ˜¯æ˜ å°„çŸ©é˜µï¼Œh æ˜¯æ³¨æ„åŠ›å¤´çš„æ•°é‡

- æœ€åï¼Œå°†å¤šå¤´çš„ç»“æœæ‹¼æ¥èµ·æ¥å°±å¾—åˆ°æœ€ç»ˆ $ğ‘šÃ—â„ğ‘‘^~_ğ‘£$ çš„ç»“æœåºåˆ—
- æ‰€è°“çš„â€œå¤šå¤´â€ (Multi-head)ï¼Œå…¶å®å°±æ˜¯å¤šåšå‡ æ¬¡ Scaled Dot-product Attentionï¼Œç„¶åæŠŠç»“æœæ‹¼æ¥



<img src="../../pics/turn_transformer/turn_5.png" width=400 align=left>

```python
#ä»£ç ï¼šhttps://transformers.run/c1/attention/#multi-head-attention

# æ¯ä¸ªå¤´éƒ½ä¼šåˆå§‹åŒ–ä¸‰ä¸ªç‹¬ç«‹çš„çº¿æ€§å±‚ï¼Œè´Ÿè´£å°† Q, K, V åºåˆ—æ˜ å°„åˆ°å°ºå¯¸ä¸º [batch_size, seq_len, head_dim] çš„å¼ é‡
#å…¶ä¸­ head_dim æ˜¯æ˜ å°„åˆ°çš„å‘é‡ç»´åº¦


#æœ€ååªéœ€è¦æ‹¼æ¥å¤šä¸ªæ³¨æ„åŠ›å¤´çš„è¾“å‡ºå°±å¯ä»¥æ„å»ºå‡º Multi-head Attention å±‚
#è¿™é‡Œåœ¨æ‹¼æ¥åè¿˜é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢æ¥ç”Ÿæˆæœ€ç»ˆçš„è¾“å‡ºå¼ é‡

#è¿™é‡Œä½¿ç”¨ BERT-base-uncased æ¨¡å‹çš„å‚æ•°åˆå§‹åŒ– Multi-head Attention å±‚ï¼Œå¹¶ä¸”å°†ä¹‹å‰æ„å»ºçš„è¾“å…¥é€å…¥æ¨¡å‹ä»¥éªŒè¯æ˜¯å¦å·¥ä½œæ­£å¸¸
```

### 2.3 Transformer Encoder

#### (1) ç®€ä»‹

æ ‡å‡† Transformer ç»“æ„ï¼šEncoder è´Ÿè´£å°†è¾“å…¥çš„è¯è¯­åºåˆ—è½¬æ¢ä¸ºè¯å‘é‡åºåˆ—ï¼ŒDecoder åˆ™åŸºäº Encoder çš„éšçŠ¶æ€æ¥è¿­ä»£åœ°ç”Ÿæˆè¯è¯­åºåˆ—ä½œä¸ºè¾“å‡ºï¼Œæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªè¯è¯­

<img src="../../pics/turn_transformer/turn_6.png" width=500 align=left>

å…¶ä¸­ï¼ŒEncoder å’Œ Decoder éƒ½å„è‡ªåŒ…å«æœ‰å¤šä¸ª building blocksï¼Œä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡çš„ä¾‹å­ï¼š

- è¾“å…¥çš„è¯è¯­é¦–å…ˆè¢«è½¬æ¢ä¸ºè¯å‘é‡

    > ç”±äºæ³¨æ„åŠ›æœºåˆ¶æ— æ³•æ•è·è¯è¯­ä¹‹é—´çš„ä½ç½®å…³ç³»ï¼Œå› æ­¤è¿˜é€šè¿‡ positional embeddings å‘è¾“å…¥ä¸­æ·»åŠ ä½ç½®ä¿¡æ¯

- Encoder ç”±ä¸€å † encoder layers (blocks) ç»„æˆï¼Œç±»ä¼¼äºå›¾åƒé¢†åŸŸä¸­çš„å †å å·ç§¯å±‚

    Decoder ä¸­ä¹ŸåŒ…å«æœ‰å †å çš„ decoder layersï¼›

- Encoder çš„è¾“å‡ºè¢«é€å…¥åˆ° Decoder å±‚ä¸­ä»¥é¢„æµ‹æ¦‚ç‡æœ€å¤§çš„ä¸‹ä¸€ä¸ªè¯

- ç„¶åå½“å‰çš„è¯è¯­åºåˆ—åˆè¢«é€å›åˆ° Decoder ä¸­ä»¥ç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ï¼Œé‡å¤ç›´è‡³å‡ºç°åºåˆ—ç»“æŸç¬¦ EOS æˆ–è¶…è¿‡æœ€å¤§è¾“å‡ºé•¿åº¦

<img src="../../pics/turn_transformer/turn_7.png" width=900 align=left>

#### (2) The Feed-Forward Layer

**å‰é¦ˆå­å±‚(The Feed-Forward Layer/position-wise feed-forward layer)**ï¼šå®é™…ä¸Šæ˜¯ä¸¤å±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œå…¶å•ç‹¬åœ°å¤„ç†åºåˆ—ä¸­çš„æ¯ä¸€ä¸ªè¯å‘é‡

> å¸¸è§åšæ³•æ˜¯è®©ç¬¬ä¸€å±‚çš„ç»´åº¦æ˜¯è¯å‘é‡å¤§å°çš„ 4 å€ï¼Œç„¶åä»¥ GELU ä½œä¸ºæ¿€æ´»å‡½æ•°

```python
import torch.nn as nn

class FeedForwar(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)
        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        
    def forward(self, x):
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        x = self.dropout(x)
        return x
    
#æµ‹è¯•
feed_forward = FeedForward(config)
ff_outputs = feed_forward(attn_output)
print(ff_outputs.size())
```

è‡³æ­¤åˆ›å»ºå®Œæ•´ Transformer Encoder çš„æ‰€æœ‰è¦ç´ éƒ½å·²é½å¤‡ï¼Œåªéœ€è¦å†åŠ ä¸Š Skip Connections å’Œ Layer Normalization å°±å¤§åŠŸå‘Šæˆäº†

#### (3) Layer Normalization

Layer Normalization è´Ÿè´£å°†ä¸€æ‰¹ (batch) è¾“å…¥ä¸­çš„æ¯ä¸€ä¸ªéƒ½æ ‡å‡†åŒ–ä¸ºå‡å€¼ä¸ºé›¶ä¸”å…·æœ‰å•ä½æ–¹å·®ï¼›

Skip Connections åˆ™æ˜¯å°†å¼ é‡ç›´æ¥ä¼ é€’ç»™æ¨¡å‹çš„ä¸‹ä¸€å±‚è€Œä¸è¿›è¡Œå¤„ç†ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°å¤„ç†åçš„å¼ é‡ä¸­

---

å‘ Transformer Encoder/Decoder ä¸­æ·»åŠ  Layer Normalization ç›®å‰å…±æœ‰ä¸¤ç§åšæ³•ï¼š

- **Post layer normalization**(Transformer è®ºæ–‡ä¸­ä½¿ç”¨çš„æ–¹å¼)ï¼šå°† Layer normalization æ”¾åœ¨ Skip Connections ä¹‹é—´ã€‚

    > ä½†æ˜¯å› ä¸ºæ¢¯åº¦å¯èƒ½ä¼šå‘æ•£ï¼Œè¿™ç§åšæ³•å¾ˆéš¾è®­ç»ƒï¼Œè¿˜éœ€è¦ç»“åˆå­¦ä¹ ç‡é¢„çƒ­ (learning rate warm-up) ç­‰æŠ€å·§

- **Pre layer normalization**(ä¸»æµåšæ³•)ï¼šå°† Layer Normalization æ”¾ç½®äº Skip Connections çš„èŒƒå›´å†…

    > è¿™ç§åšæ³•é€šå¸¸è®­ç»ƒè¿‡ç¨‹ä¼šæ›´åŠ ç¨³å®šï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•å­¦ä¹ ç‡é¢„çƒ­

<img src="../../pics/turn_transformer/turn_8.png" width=700 align=left>

é‡‡ç”¨ç¬¬äºŒç§æ–¹å¼æ¥æ„å»º Transformer Encoder å±‚ï¼š

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)
        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)
        self.attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)

    def forward(self, x, mask=None):
        # Apply layer normalization and then copy input into query, key, value
        hidden_state = self.layer_norm_1(x)
        # Apply attention with a skip connection
        x = x + self.attention(hidden_state, hidden_state, hidden_state, mask=mask)
        # Apply feed-forward layer with a skip connection
        x = x + self.feed_forward(self.layer_norm_2(x))
        return x

#å°†ä¹‹å‰æ„å»ºçš„è¾“å…¥é€å…¥åˆ°è¯¥å±‚ä¸­è¿›è¡Œæµ‹è¯•ï¼š
encoder_layer = TransformerEncoderLayer(config)
print(inputs_embeds.shape)
print(encoder_layer(inputs_embeds).size())

#è¾“å‡º
torch.Size([1, 5, 768])
torch.Size([1, 5, 768])
```

#### (4) Positional Embeddings

> ç”±äºæ³¨æ„åŠ›æœºåˆ¶æ— æ³•æ•è·è¯è¯­ä¹‹é—´çš„ä½ç½®ä¿¡æ¯ï¼Œå› æ­¤ Transformer æ¨¡å‹è¿˜ä½¿ç”¨ Positional Embeddings æ·»åŠ äº†è¯è¯­çš„ä½ç½®ä¿¡æ¯

Positional Embeddings åŸºäºä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„æƒ³æ³•ï¼š**ä½¿ç”¨ä¸ä½ç½®ç›¸å…³çš„å€¼æ¨¡å¼æ¥å¢å¼ºè¯å‘é‡** 

---

Positional Embeddings çš„æ–¹æ¡ˆï¼š

- **è®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ ä½ç½®åµŒå…¥**ï¼šå½“é¢„è®­ç»ƒæ•°æ®é›†è¶³å¤Ÿå¤§æ—¶é‡‡ç”¨

- **ç»å¯¹ä½ç½®è¡¨ç¤º**ï¼šä½¿ç”¨ç”±è°ƒåˆ¶çš„æ­£å¼¦å’Œä½™å¼¦ä¿¡å·ç»„æˆçš„é™æ€æ¨¡å¼æ¥ç¼–ç ä½ç½®ï¼Œå½“æ²¡æœ‰å¤§é‡è®­ç»ƒæ•°æ®å¯ç”¨æ—¶ï¼Œè¿™ç§æ–¹æ³•å°¤å…¶æœ‰æ•ˆï¼›

- **ç›¸å¯¹ä½ç½®è¡¨ç¤º**ï¼šåœ¨ç”ŸæˆæŸä¸ªè¯è¯­çš„è¯å‘é‡æ—¶ï¼Œä¸€èˆ¬è·ç¦»å®ƒè¿‘çš„è¯è¯­æ›´ä¸ºé‡è¦ï¼Œå› æ­¤ä¹Ÿæœ‰å·¥ä½œé‡‡ç”¨ç›¸å¯¹ä½ç½®ç¼–ç 

    > å› ä¸ºæ¯ä¸ªè¯è¯­çš„ç›¸å¯¹åµŒå…¥ä¼šæ ¹æ®åºåˆ—çš„ä½ç½®è€Œå˜åŒ–ï¼Œè¿™éœ€è¦åœ¨æ¨¡å‹å±‚é¢å¯¹æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ˜¯é€šè¿‡å¼•å…¥åµŒå…¥å±‚æ¥å®Œæˆï¼Œä¾‹å¦‚ DeBERTa ç­‰æ¨¡å‹

---

ä½¿ç”¨â€œè®©æ¨¡å‹è‡ªåŠ¨å­¦ä¹ ä½ç½®åµŒå…¥â€çš„æ–¹å¼åˆ›å»ºè‡ªå®šä¹‰çš„ Embeddings æ¨¡å—ï¼ŒåŒæ—¶å°†è¯è¯­å’Œä½ç½®æ˜ å°„åˆ°åµŒå…¥å¼è¡¨ç¤ºï¼Œæœ€ç»ˆè¾“å‡ºä¸¤ä¸ªè¡¨ç¤ºä¹‹å’Œ

```python
class Embeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config.vocab_size,
                                             config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings,
                                                config.hidden_size)
        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        # Create position IDs for input sequence
        seq_length = input_ids.size(1)
        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
        # Create token and position embeddings
        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)
        # Combine token and position embeddings
        embeddings = token_embeddings + position_embeddings
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

#æµ‹è¯•
embedding_layer = Embeddings(config)
print(embedding_layer(inputs.input_ids).size())

#è¾“å‡º
torch.Size([1, 5, 768])
```

#### (5) å®Œæ•´çš„ Transformer Encoder ä»£ç 

``` python
class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.embeddings = Embeddings(config)
        self.layers = nn.ModuleList([TransformerEncoderLayer(config)
                                     for _ in range(config.num_hidden_layers)])

    def forward(self, x, mask=None):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, mask=mask)
        return x

#æµ‹è¯•
encoder = TransformerEncoder(config)
print(encoder(inputs.input_ids).size())

#è¾“å‡º
torch.Size([1, 5, 768])
```

### 2.4 Transformer Decoder

Transformer Decoder ä¸ Encoder æœ€å¤§çš„ä¸åŒåœ¨äº **Decoder æœ‰ä¸¤ä¸ªæ³¨æ„åŠ›å­å±‚**

<img src="../../pics/turn_transformer/turn_9.png" width=900 align=left>

- **Masked multi-head self-attention layer**ï¼šç¡®ä¿åœ¨æ¯ä¸ªæ—¶é—´æ­¥ç”Ÿæˆçš„è¯è¯­ä»…åŸºäºè¿‡å»çš„è¾“å‡ºå’Œå½“å‰é¢„æµ‹çš„è¯ï¼Œå¦åˆ™ Decoder ç›¸å½“äºä½œå¼Šäº†ï¼›

- **Encoder-decoder attention layer**ï¼šä»¥è§£ç å™¨çš„ä¸­é—´è¡¨ç¤ºä½œä¸º queriesï¼Œå¯¹ encoder stack çš„è¾“å‡º key å’Œ value å‘é‡æ‰§è¡Œ Multi-head Attention

    > é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒEncoder-Decoder Attention Layer å°±å¯ä»¥å­¦ä¹ åˆ°å¦‚ä½•å…³è”æ¥è‡ªä¸¤ä¸ªä¸åŒåºåˆ—çš„è¯è¯­ï¼Œä¾‹å¦‚ä¸¤ç§ä¸åŒçš„è¯­è¨€
    >
    > è§£ç å™¨å¯ä»¥è®¿é—®æ¯ä¸ª block ä¸­ Encoder çš„ keys å’Œ valuesã€‚

---

ä¸ Encoder ä¸­çš„ Mask ä¸åŒï¼ŒDecoder çš„ Mask æ˜¯ä¸€ä¸ªä¸‹ä¸‰è§’çŸ©é˜µï¼š

```python
#ä½¿ç”¨ PyTorch è‡ªå¸¦çš„ tril() å‡½æ•°æ¥åˆ›å»ºä¸‹ä¸‰è§’çŸ©é˜µ
seq_len = inputs.input_ids.size(-1)
mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)
print(mask[0])

#è¾“å‡º
tensor([[1., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0.],
        [1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1.]])

#é€šè¿‡ Tensor.masked_fill() å°†æ‰€æœ‰é›¶æ›¿æ¢ä¸ºè´Ÿæ— ç©·å¤§æ¥é˜²æ­¢æ³¨æ„åŠ›å¤´çœ‹åˆ°æœªæ¥çš„è¯è¯­è€Œé€ æˆä¿¡æ¯æ³„éœ²
scores.masked_fill(mask == 0, -float("inf"))

#è¾“å‡º
tensor([[[26.8082,    -inf,    -inf,    -inf,    -inf],
         [-0.6981, 26.9043,    -inf,    -inf,    -inf],
         [-2.3190,  1.2928, 27.8710,    -inf,    -inf],
         [-0.5897,  0.3497, -0.3807, 27.5488,    -inf],
         [ 0.5275,  2.0493, -0.4869,  1.6100, 29.0893]]],
       grad_fn=<MaskedFillBackward0>)
```

# ä¸‰ã€pipelines

### 3.1 ç®€ä»‹

Transformers åº“å°†ç›®å‰çš„ NLP ä»»åŠ¡å½’çº³ä¸ºå‡ ä¸‹å‡ ç±»ï¼š

- **æ–‡æœ¬åˆ†ç±»ï¼š**ä¾‹å¦‚æƒ…æ„Ÿåˆ†æã€å¥å­å¯¹å…³ç³»åˆ¤æ–­ç­‰ï¼›
- **å¯¹æ–‡æœ¬ä¸­çš„è¯è¯­è¿›è¡Œåˆ†ç±»ï¼š**ä¾‹å¦‚è¯æ€§æ ‡æ³¨ (POS)ã€å‘½åå®ä½“è¯†åˆ« (NER) ç­‰ï¼›
- **æ–‡æœ¬ç”Ÿæˆï¼š**ä¾‹å¦‚å¡«å……é¢„è®¾çš„æ¨¡æ¿ (prompt)ã€é¢„æµ‹æ–‡æœ¬ä¸­è¢«é®æ©æ‰ (masked) çš„è¯è¯­ï¼›
- **ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼š**ä¾‹å¦‚æ ¹æ®ç»™å®šçš„é—®é¢˜ä»ä¸€æ®µæ–‡æœ¬ä¸­æŠ½å–å‡ºå¯¹åº”çš„ç­”æ¡ˆï¼›
- **æ ¹æ®è¾“å…¥æ–‡æœ¬ç”Ÿæˆæ–°çš„å¥å­ï¼š**ä¾‹å¦‚æ–‡æœ¬ç¿»è¯‘ã€è‡ªåŠ¨æ‘˜è¦ç­‰ã€‚

---

Transformers åº“æœ€åŸºç¡€çš„å¯¹è±¡å°±æ˜¯ `pipeline()` å‡½æ•°ï¼Œå®ƒå°è£…äº†é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„å‰å¤„ç†å’Œåå¤„ç†ç¯èŠ‚

åªéœ€è¾“å…¥æ–‡æœ¬ï¼Œå°±èƒ½å¾—åˆ°é¢„æœŸçš„ç­”æ¡ˆï¼Œç›®å‰å¸¸ç”¨çš„ [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) æœ‰ï¼š

- `feature-extraction` ï¼ˆè·å¾—æ–‡æœ¬çš„å‘é‡åŒ–è¡¨ç¤ºï¼‰
- `fill-mask` ï¼ˆå¡«å……è¢«é®ç›–çš„è¯ã€ç‰‡æ®µï¼‰
- `ner`ï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰
- `question-answering` ï¼ˆè‡ªåŠ¨é—®ç­”ï¼‰
- `sentiment-analysis` ï¼ˆæƒ…æ„Ÿåˆ†æï¼‰
- `summarization` ï¼ˆè‡ªåŠ¨æ‘˜è¦ï¼‰
- `text-generation` ï¼ˆæ–‡æœ¬ç”Ÿæˆï¼‰
- `translation` ï¼ˆæœºå™¨ç¿»è¯‘ï¼‰
- `zero-shot-classification` ï¼ˆé›¶è®­ç»ƒæ ·æœ¬åˆ†ç±»ï¼‰

### 3.2 pipeline åŸç†

ä»¥ç¬¬ä¸€ä¸ªæƒ…æ„Ÿåˆ†æ pipeline ä¸ºä¾‹ï¼š

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I've been waiting for a HuggingFace course my whole life.")
print(result)

#ç»“æœ
[{'label': 'POSITIVE', 'score': 0.9598048329353333}]
```

å®é™…ä¸Šå®ƒçš„èƒŒåç»è¿‡äº†ä¸‰ä¸ªæ­¥éª¤ï¼š

1. **é¢„å¤„ç† (preprocessing)**ï¼Œå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼
2. **å°†å¤„ç†å¥½çš„è¾“å…¥é€å…¥æ¨¡å‹** 
3. **å¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œåå¤„ç† (postprocessing)**ï¼Œå°†å…¶è½¬æ¢ä¸ºäººç±»æ–¹ä¾¿é˜…è¯»çš„æ ¼å¼

<img src="../../pics/turn_transformer/turn_10.png" width=800 align=left>

#### (1) ä½¿ç”¨åˆ†è¯å™¨è¿›è¡Œé¢„å¤„ç†

> å› ä¸ºç¥ç»ç½‘ç»œæ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†æ–‡æœ¬ï¼Œå› æ­¤é¦–å…ˆéœ€è¦é€šè¿‡**é¢„å¤„ç†**ç¯èŠ‚å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—

ä½¿ç”¨æ¯ä¸ªæ¨¡å‹å¯¹åº”çš„åˆ†è¯å™¨ (tokenizer) æ¥è¿›è¡Œï¼š

1. å°†è¾“å…¥åˆ‡åˆ†ä¸ºè¯è¯­ã€å­è¯æˆ–è€…ç¬¦å·ï¼ˆä¾‹å¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç»Ÿç§°ä¸º **tokens**ï¼›
2. æ ¹æ®æ¨¡å‹çš„è¯è¡¨å°†æ¯ä¸ª token æ˜ å°„åˆ°å¯¹åº”çš„ token ç¼–å·ï¼ˆå°±æ˜¯ä¸€ä¸ªæ•°å­—ï¼‰ï¼›
3. æ ¹æ®æ¨¡å‹çš„éœ€è¦ï¼Œæ·»åŠ ä¸€äº›é¢å¤–çš„è¾“å…¥

`AutoTokenizer` ç±»å’Œ `from_pretrained()` å‡½æ•°å¯ä»¥è‡ªåŠ¨æ ¹æ®æ¨¡å‹ checkpoint åç§°æ¥è·å–å¯¹åº”çš„åˆ†è¯å™¨ï¼š

> å¯¹è¾“å…¥æ–‡æœ¬çš„é¢„å¤„ç†éœ€è¦ä¸æ¨¡å‹è‡ªèº«é¢„è®­ç»ƒæ—¶çš„æ“ä½œå®Œå…¨ä¸€è‡´ï¼Œåªæœ‰è¿™æ ·æ¨¡å‹æ‰å¯ä»¥æ­£å¸¸åœ°å·¥ä½œ







#### (2) å°†é¢„å¤„ç†å¥½çš„è¾“å…¥é€å…¥æ¨¡å‹







#### (3) å¯¹æ¨¡å‹è¾“å‡ºè¿›è¡Œåå¤„ç†







### 3.3 pipeline å‡½æ•°

#### (1) æƒ…æ„Ÿåˆ†æ pipeline





#### (2) é›¶è®­ç»ƒæ ·æœ¬åˆ†ç±» pipeline





#### (3) æ–‡æœ¬ç”Ÿæˆ pipeline





#### (4) é®ç›–è¯å¡«å…… pipeline







#### (5) å‘½åå®ä½“è¯†åˆ« pipeline







#### (6) è‡ªåŠ¨é—®ç­” pipeline







#### (7) è‡ªåŠ¨æ‘˜è¦ pipeline





# å››ã€æ¨¡å‹ä¸åˆ†è¯å™¨

### 4.1 æ¨¡å‹







### 4.2 åˆ†è¯å™¨







### 4.3 å¤„ç†å¤šæ®µæ–‡æœ¬







### 4.4 æ·»åŠ  Token







### 4.5 Token Embedding åˆå§‹åŒ–







# äº”ã€ä»»åŠ¡æ¡ˆä¾‹

### 5.1 åºåˆ—æ ‡æ³¨ä»»åŠ¡





### 5.2 æŠ½å–å¼é—®ç­”ä»»åŠ¡





### 5.3 ç¿»è¯‘ä»»åŠ¡







### 5.4 æ–‡æœ¬æ‘˜è¦ä»»åŠ¡







### 5.5 æƒ…æ„Ÿåˆ†æä»»åŠ¡









